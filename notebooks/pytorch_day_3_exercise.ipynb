{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712f0b95",
   "metadata": {},
   "source": [
    "## ðŸ“… Day 3: Define Model with nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c50659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=2, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5e8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create a deeper model with dropout and batchnorm\n",
    "class deeperMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128,8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2737d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of deeperMLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# - Print model parameters (weights & biases)\n",
    "model = deeperMLP()\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189b9d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5258,  0.0107,  0.1155,  0.5611, -0.9563,  0.4834,  0.0225,  0.9426],\n",
       "        [-0.4578,  0.7084,  0.8889,  0.0994, -0.4184, -0.7783,  0.1204, -0.4687],\n",
       "        [-0.4432, -0.2854,  0.4332,  1.2963, -1.6711,  0.6954,  0.7195, -0.8587],\n",
       "        [ 0.0763,  0.1265,  0.1029,  0.1823, -0.9249,  0.7460,  0.0136, -0.1553],\n",
       "        [ 0.2384,  0.1753, -0.1287,  0.4755, -1.0940,  0.0910, -0.3139,  0.8647]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Forward random input and inspect output shape\n",
    "import torch\n",
    "x = torch.randn((5,512))\n",
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366e8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Write a custom forward method with skip connection\n",
    "# Pre-activation residual (common in modern nets/Transformers):\n",
    "# Norm â†’ Activation â†’ Linear â†’ Norm â†’ Activation â†’ Linear â†’ (add skip)\n",
    "# Pre-activation often trains a bit more smoothly.\n",
    "class oneResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64, 128, bias=False),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.layers(x) + x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Residual Block ----------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim=64, hidden=128, p_drop=0.2, pre_norm=False, scale_residual=1.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden = hidden\n",
    "        self.pre_norm = pre_norm\n",
    "        self.scale = scale_residual\n",
    "        # Post-norm: Linear â†’ Norm â†’ â€¦ â†’ set bias=False. Pre-norm: Norm â†’ Linear â†’ â€¦ â†’ set bias=True.\n",
    "        # The norm layer handles the shift/scale\n",
    "        self.linear_0 = nn.Linear(dim, hidden, bias=pre_norm)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop_0 = nn.Dropout(p_drop)\n",
    "        self.linear_1 = nn.Linear(hidden, dim)\n",
    "\n",
    "        self.norm_0 = nn.LayerNorm(self.dim)\n",
    "        self.norm_1 = nn.LayerNorm(self.hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pre-norm: x â†’ LN â†’ fc1 â†’ Activation â†’ Dropout â†’ LN â†’ fc2 â†’ + skip\n",
    "        # post-noem x â†’ fc1 â†’ LN â†’ Activation â†’ Dropout â†’ fc2 â†’ + skip\n",
    "        if self.pre_norm:\n",
    "            h = self.norm_0(x)\n",
    "            h = self.linear_0(h)\n",
    "        else:\n",
    "            h = self.linear_0(x)\n",
    "            h = self.norm_1(h)\n",
    "        h = self.activation(h)\n",
    "        h = self.drop_1(h)\n",
    "        y = self.linear_1(h)\n",
    "        y = y * self.scale + x\n",
    "        return y  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3276f6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Multi-Block Residual MLP ----------\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, in_dim=64, working_dim = 256, hidden=512, num_blocks=4, p_drop=0.2, pre_norm=False, out_dim=1):\n",
    "        super().__init__()\n",
    "        # Raw input (size in_dim) â†’ Stem (projects to working_dim) â†’ Residual blocks â†’ Final head\n",
    "        self.stem = nn.Linear(in_dim, working_dim, bias=False)\n",
    "        self.stem_norm = nn.LayerNorm(working_dim)\n",
    "        self.stem_act  = nn.ReLU()\n",
    "        self.pre_norm = pre_norm\n",
    "        if num_blocks>8:\n",
    "            scale_residual=0.1\n",
    "        else:\n",
    "            scale_residual=1\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(working_dim, hidden, p_drop, pre_norm, scale_residual) for _ in range(num_blocks)])\n",
    "        self.ln = nn.LayerNorm(working_dim)\n",
    "        self.final = nn.Linear(working_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.stem(x)\n",
    "        if not self.pre_norm:\n",
    "            h = self.stem_norm(h)\n",
    "            h = self.stem_act(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        y = self.ln(h)\n",
    "        y = self.final(y)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
