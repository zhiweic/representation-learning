{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5288de6",
   "metadata": {},
   "source": [
    "# RoPE + Relative Bias in MHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264104b",
   "metadata": {},
   "source": [
    "A manual MHA that supports: <br>\n",
    "\n",
    "- Sinusoidal add (absolute)\n",
    "\n",
    "- RoPE (rotary) on Q/K only\n",
    "\n",
    "- T5-style relative position bias added to scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68083318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else (\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ad47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Masks --------------------\n",
    "def make_padding_mask(lengths, T):\n",
    "    rng = torch.arange(T, device=lengths.device).unsqueeze(0)       # [1,T]\n",
    "    return (rng < lengths.unsqueeze(1))                             # [B,T] True=real\n",
    "\n",
    "def make_causal_mask(T, device):\n",
    "    i = torch.arange(T, device=device).unsqueeze(1)  # [T,1]\n",
    "    j = torch.arange(T, device=device).unsqueeze(0)  # [1,T]\n",
    "    return (j <= i)                                  # [T,T] True=keep (no future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7694f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Sinusoidal positions (absolute) --------------------\n",
    "def sinusoidal_positions(T, d_model, device, base=10000.0):\n",
    "    pos = torch.arange(T, device=device).float()             # [T]\n",
    "    i = torch.arange(0, d_model, 2, device=device).float()   # [D/2]\n",
    "    inv_freq = torch.exp(-math.log(base)*(i / d_model))      # [D/2]\n",
    "    emb = torch.zeros(T, d_model, device=device)\n",
    "    angles = pos.unsqueeze(1) * inv_freq # [T, 1] * [1, D/2] = [T, D/2] \n",
    "    emb[:, 0::2] = torch.sin(angles)\n",
    "    emb[:, 1::2] = torch.cos(angles)\n",
    "    return emb                                              # [T, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5450bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- RoPE cache & apply (for Q/K) --------------------\n",
    "def rope_cache(T, dim, device, base=10000.0):\n",
    "    assert dim % 2 == 0, \"RoPE dimension must be even\"\n",
    "    half = dim // 2\n",
    "    idx = torch.arange(0, half, device=device).float()\n",
    "    inv_freq = torch.exp(-math.log(base) * (idx / half))        # [half]\n",
    "    pos = torch.arange(T, device=device).float()   # [T]\n",
    "    # Einstein summation notation: torch.einsum(\"ij,jk->ik\", A, B) performs matrix multiplication of tensors A and B\n",
    "    freqs = torch.einsum(\"t,f->tf\", pos, inv_freq) # [T, half] outer product\n",
    "    return torch.sin(freqs), torch.cos(freqs)      # both [T, half]\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: [B,H,T,Dh]; sin,cos: [T, Dh/2]\n",
    "    returns x_rot: [B,H,T,Dh]\n",
    "    \"\"\"\n",
    "    B,H,T,Dh = x.shape\n",
    "    half = Dh // 2\n",
    "    x1, x2 = x[..., :half], x[..., half:] # [B, H, T, Dh/2]\n",
    "    # broadcast sin/cos: [1,1,T,half]\n",
    "    sin_ = sin.view(1,1,T,half)\n",
    "    cos_ = cos.view(1,1,T,half)\n",
    "    xr1 = x1 * cos_ - x2 * sin_ # [B, H, T, Dh/2]\n",
    "    xr2 = x1 * sin_ + x2 * cos_ # [B, H, T, Dh/2]\n",
    "    return torch.cat([xr1, xr2], dim=-1) # [B,H,T,half] + [B,H,T,half] -> [B,H,T,Dh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263ca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- T5-style Relative Position Bias --------------------\n",
    "class RelativePositionBias(nn.Module):\n",
    "    \"\"\"\n",
    "    T5-style: learn per-head scalar biases for relative offsets, clipped to [-max_rel+1, max_rel-1].\n",
    "    Added *to attention scores* before softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int, max_rel: int = 128):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.max_rel = max_rel\n",
    "        # We learn a scalar bias per head, per relative offset Δ in the range Δ∈{−R+1,…,0,…,+R−1}, where R=max_rel.\n",
    "        self.table = nn.Parameter(torch.zeros(num_heads, 2*max_rel - 1))  # [H, 2R-1]\n",
    "        nn.init.zeros_(self.table)  # small init is fine\n",
    "\n",
    "    def forward(self, T: int, device):\n",
    "        # rel[i,j] = (j - i) clipped into [-R+1, R-1], then shifted to [0, 2R-2]\n",
    "        q_pos = torch.arange(T, device=device)[:, None] # [T,1]  (query indices i)\n",
    "        k_pos = torch.arange(T, device=device)[None, :] # [1,T]  (key indices   j)\n",
    "        # Clipping: distances beyond ±(R−1) get folded into the border bucket.\n",
    "        # Shift: map offsets from [−(R−1),…,+(R−1)] to indices [0,…,2R−2]\n",
    "        rel = (k_pos - q_pos).clamp(-self.max_rel+1, self.max_rel-1) + (self.max_rel - 1)  # [T,T]\n",
    "        # Advanced indexing: for each head h, and each pair (i,j), we look up the scalar table[h, bucket(j-i)]\n",
    "        bias = self.table[:, rel]  # [H, T, T]\n",
    "        return bias.unsqueeze(0)   # [1, H, T, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054db1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Manual MHA with options --------------------\n",
    "class MHALayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int,\n",
    "                 p_drop: float = 0.0,\n",
    "                 posenc: str = \"none\",          # {\"none\",\"sin\",\"rope\"}\n",
    "                 use_relbias: bool = False,\n",
    "                 relbias_max_rel: int = 128):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.h = num_heads\n",
    "        self.dh = d_model // num_heads\n",
    "        self.posenc = posenc\n",
    "        self.use_relbias = use_relbias\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.scale = math.sqrt(self.dh)\n",
    "        if use_relbias:\n",
    "            self.relbias = RelativePositionBias(num_heads, max_rel=relbias_max_rel)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        B,T,D = x.shape\n",
    "        return x.view(B, T, self.h, self.dh).permute(0,2,1,3)  # [B,H,T,Dh]\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        B,H,T,Dh = x.shape\n",
    "        return x.permute(0,2,1,3).contiguous().view(B, T, H*Dh) # [B,T,D]\n",
    "\n",
    "    def forward(self, x, pad_mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        x: [B,T,D]; pad_mask: [B,T] True=real, False=PAD\n",
    "        \"\"\"\n",
    "        B,T,D = x.shape\n",
    "        # (A) absolute sinusoid add BEFORE projections (classic)\n",
    "        if self.posenc == \"sin\":\n",
    "            x = x + sinusoidal_positions(T, D, x.device)\n",
    "\n",
    "        Q = self.split_heads(self.Wq(x))  # [B,H,T,Dh]\n",
    "        K = self.split_heads(self.Wk(x))  # [B,H,T,Dh]\n",
    "        V = self.split_heads(self.Wv(x))  # [B,H,T,Dh]\n",
    "\n",
    "        # (B) RoPE rotates Q,K (NOT V)\n",
    "        if self.posenc == \"rope\":\n",
    "            sin, cos = rope_cache(T, self.dh, x.device)\n",
    "            Q = apply_rope(Q, sin, cos)\n",
    "            K = apply_rope(K, sin, cos)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))/self.scale # [B, H, T, T]\n",
    "\n",
    "        # (C) T5-style relative bias\n",
    "        if self.use_relbias:\n",
    "            scores = scores + self.relbias(T, x.device)  # broadcast over batch\n",
    "\n",
    "        # padding mask (mask keys): [B,1,1,T]\n",
    "        if pad_mask is not None:\n",
    "            scores = scores.masked_fill(~pad_mask[:, None, None, :], float(\"-inf\"))\n",
    "\n",
    "        # causal mask: [1,1,T,T]\n",
    "        if causal:\n",
    "            cm = make_causal_mask(T, x.device)\n",
    "            scores = scores.masked_fill(~cm[None, None, :, :], float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)  # [B,H,T,T]\n",
    "        attn = self.drop(attn)\n",
    "        ctx = torch.matmul(attn, V)           # [B,H,T,Dh]\n",
    "        out = self.Wo(self.merge_heads(ctx)) # [B,T,D]\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a7c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Pre-LN Encoder Block --------------------\n",
    "class PreLNEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model:int, num_heads:int, p_drop:float=0.1,\n",
    "                 posenc:str=\"none\", use_relbias:bool=False, relbias_max_rel:int=128, ff_mult:int=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MHALayer(d_model, num_heads, p_drop, posenc, use_relbias, relbias_max_rel)\n",
    "        self.drop1 = nn.Dropout(p_drop)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_mult*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_mult*d_model, d_model)\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x, pad_mask=None, causal=False):\n",
    "        a, attn = self.mha(self.ln1(x), pad_mask=pad_mask, causal=causal)\n",
    "        x = x + self.drop1(a)              # residual after MHA\n",
    "        f = self.ffn(self.ln2(x))\n",
    "        x = x + self.drop2(f)              # residual after FFN\n",
    "        return x, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1c7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, T, D, H = 2, 8, 32, 4\n",
    "lengths = torch.tensor([8, 5], device=DEVICE)\n",
    "pad_mask = make_padding_mask(lengths, T).to(DEVICE)\n",
    "x = torch.randn(B, T, D, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f4cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out shape: (2, 8, 32) attn shape: (2, 4, 8, 8)\n",
      "attn[1,0,-1] (last query, head0) to PAD keys ~0: [0.18280170857906342, 0.23103535175323486, 0.2453910857439041, 0.19526836276054382, 0.145503431558609, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# == No posenc, no relbias, encoder mask ==\n",
    "blk = PreLNEncoderBlock(D, H, p_drop=0.0, posenc=\"none\", use_relbias=False).to(DEVICE)\n",
    "y, A = blk(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"out shape:\", tuple(y.shape), \"attn shape:\", tuple(A.shape))\n",
    "print(\"attn[1,0,-1] (last query, head0) to PAD keys ~0:\", A[1,0,-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f21967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinusoid attn[0,0,3] row: [0.1082, 0.1355, 0.1053, 0.1111, 0.0876, 0.1482, 0.1909, 0.1131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hg/7ml395w169dbrcxqhmh9wfmw0000gn/T/ipykernel_28539/1345926730.py:4: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  print(\"Sinusoid attn[0,0,3] row:\", [round(float(v),4) for v in A[0,0,3]])\n"
     ]
    }
   ],
   "source": [
    "# == sinusoidal Pos Enc ==\n",
    "blk = PreLNEncoderBlock(D, H, p_drop=0.0, posenc=\"sin\", use_relbias=False).to(DEVICE)\n",
    "y, A = blk(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"Sinusoid attn[0,0,3] row:\", [round(float(v),4) for v in A[0,0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e90851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rope attn[0,0,3] row: [0.1218, 0.1905, 0.2053, 0.075, 0.0678, 0.1006, 0.1499, 0.0891]\n"
     ]
    }
   ],
   "source": [
    "# == Rope Pos Enc ==\n",
    "blk = PreLNEncoderBlock(D, H, p_drop=0.0, posenc=\"rope\", use_relbias=False).to(DEVICE)\n",
    "y, A = blk(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"Rope attn[0,0,3] row:\", [round(float(v),4) for v in A[0,0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c22ca1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rope attn[0,0,3] row: [0.1141, 0.1353, 0.0942, 0.0912, 0.1125, 0.1933, 0.1593, 0.1001]\n"
     ]
    }
   ],
   "source": [
    "# == RoPE + T5 relative bias ==\n",
    "blk = PreLNEncoderBlock(D, H, p_drop=0.0, posenc=\"rope\", use_relbias=True).to(DEVICE)\n",
    "y, A = blk(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"Rope attn[0,0,3] row:\", [round(float(v),4) for v in A[0,0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e339415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal row t=4 (head 0), weights beyond j>4 ~0: [0.1817, 0.1905, 0.3169, 0.1942, 0.1167, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# == Decoder-style: causal + padding together ==\n",
    "blk = PreLNEncoderBlock(D, H, p_drop=0.0, posenc=\"rope\", use_relbias=True, relbias_max_rel=4).to(DEVICE)\n",
    "y, A = blk(x, pad_mask=pad_mask, causal=True)\n",
    "print(\"causal row t=4 (head 0), weights beyond j>4 ~0:\", [round(float(v),4) for v in A[0,0,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e9260",
   "metadata": {},
   "source": [
    "Sinusoidal (absolute add): add a fixed position vector to inputs; attention still uses content+absolute cues. Ties representations to absolute positions. <br>\n",
    "\n",
    "Relative position (bias on scores): add a learned bias per offset directly to logits (T5); encourages local attention or other patterns. Imposes a global tendency (e.g., strong local attention), resolving ties and stabilizing training. <br>\n",
    "\n",
    "RoPE (rotary): rotate Q/K by angle ∝ position so that their dot-product inherently depends on the relative displacement. It lets the model condition matching on distance/direction in a smooth, content-aware way. But it's only applied to Q,K, meaning that it matches by relative position, but the information retrieved (V) is the same, once matched. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
