{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02450227",
   "metadata": {},
   "source": [
    "# Drop-in SDPA MHA with optional RoPE + RelBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6631fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6f439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d98d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- (A) tiny RoPE helper -----\n",
    "def apply_rope(q, k, base=10000.0):\n",
    "    # q,k: [B,H,T,Dh]; split half-dims\n",
    "    Dh = q.size(-1); assert Dh % 2 == 0, \"RoPE needs even head dim\"\n",
    "    half = Dh // 2\n",
    "    q1, q2 = q[..., :half], q[..., half:]\n",
    "    k1, k2 = k[..., :half], k[..., half:]\n",
    "    T = q.size(-2)\n",
    "    device = q.device\n",
    "    dtype = q.dtype\n",
    "\n",
    "    pos = torch.arange(T, device=device, dtype=dtype)[:, None]                    # [T,1]\n",
    "    inv_freqs = torch.exp(-math.log(base) * (torch.arange(0, half, device=device, dtype=dtype) / half))[None, :]   # [1, half]\n",
    "    ang = pos * inv_freqs                                                    # [T,half]\n",
    "    sin, cos = torch.sin(ang), torch.cos(ang)                                              # [T,half]\n",
    "    # broadcast to [B,H,T,half]\n",
    "    sin = sin[None, None, :, :]; cos = cos[None, None, :, :]\n",
    "\n",
    "    # rotate (x1, x2) -> (x1*cos - x2*sin, x2*cos + x1*sin)\n",
    "    def rot(x1, x2):\n",
    "        return x1 * cos - x2 * sin, x2 * cos + x1 * sin\n",
    "\n",
    "    q1r, q2r = rot(q1, q2); k1r, k2r = rot(k1, k2) # [B, H, T, half]\n",
    "    q = torch.cat([q1r, q2r], dim=-1)\n",
    "    k = torch.cat([k1r, k2r], dim=-1)\n",
    "    return q, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c21c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- (B) clipped T5-style per-head relative bias -----\n",
    "class ClippedRelPosBias(nn.Module):\n",
    "    def __init__(self, num_heads, max_rel=128):\n",
    "        super().__init__()\n",
    "        self.max_rel = max_rel\n",
    "        self.table = nn.Parameter(torch.zeros(num_heads, 2*max_rel - 1))  # [H, 2R-1]\n",
    "    def forward(self, T, device=None):\n",
    "        device = device or self.table.device\n",
    "        q = torch.arange(T, device=device)[:, None] # [T,1]  (query indices i)\n",
    "        k = torch.arange(T, device=device)[None, :] # [1, T] (key indices j)\n",
    "        rel = (k - q).clamp(-self.max_rel+1, self.max_rel-1) # [T, T]\n",
    "        idx = rel + (self.max_rel - 1)              # map to [0..2R-2]\n",
    "        return self.table[:, idx]                   # [H, T, T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a396df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_neg(dtype):\n",
    "    # Safe additive mask sentinels\n",
    "    return -1e4 if dtype in (torch.float16, torch.bfloat16) else -1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29142fed",
   "metadata": {},
   "source": [
    "With nn.MultiheadAttention you get two knobs:\n",
    "\n",
    "- key_padding_mask (bool, True = PAD/disallow)\n",
    "\n",
    "- attn_mask (bool or float, broadcastable to [T,T], True = disallow for bool, added to logits for float) <br>\n",
    "\n",
    "…but F.scaled_dot_product_attention (SDPA) has one attn_mask and a different convention:\n",
    "\n",
    "- attn_mask: bool (True = ALLOW) or float (additive to logits) <br>\n",
    "\n",
    "No separate key_padding_mask\n",
    "\n",
    "So when you switch to SDPA you must fold everything (padding, causal, relative bias) into that single attn_mask. Also, on MPS/low-precision, feeding -inf can produce NaNs — prefer a large finite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54c1773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sdpa_mask(\n",
    "    pad_mask: torch.Tensor | None,  # [B,T] bool, True = real token\n",
    "    rel_bias: torch.Tensor | None,  # [H,T,T] float or None\n",
    "    *, B: int, T: int, H: int, dtype: torch.dtype, device: torch.device,\n",
    ") -> torch.Tensor | None:\n",
    "    \"\"\"\n",
    "    Returns a FLOAT additive mask for SDPA, shape [B,H,T,T], or None.\n",
    "    Combines key padding + optional relative bias. (Causal can be handled by SDPA's is_causal=True.)\n",
    "    \"\"\"\n",
    "    attn_mask = None\n",
    "\n",
    "    if pad_mask is not None and pad_mask.dtype == torch.bool:\n",
    "        # key-only mask → 0 for allowed keys, large negative for PAD keys\n",
    "        allow_keys = pad_mask[:, None, None, :]  # [B,1,1,T], True = allowed\n",
    "        neg = large_neg(dtype)\n",
    "        # convert boolean allow-mask to float additive: disallowed → a large finite negative\n",
    "        # Can't use -inf, the MPS backend turn it into nan.\n",
    "        float_mask = (~allow_keys).to(dtype=dtype) * neg\n",
    "\n",
    "    if rel_bias is not None:\n",
    "        # rel_bias should be [H,T,T]; broadcast to batch and match dtype\n",
    "        bias = rel_bias.to(dtype=dtype, device=device).unsqueeze(0).expand(B, -1, -1, -1)  # [B,H,T,T]\n",
    "        attn_mask = bias if attn_mask is None else (float_mask + bias)\n",
    "\n",
    "    return attn_mask  # float additive mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87eb1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- (C) SDPA-based MHA (pre-proj qkv, optional RoPE + RelBias) -----\n",
    "class SDPAMHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p_drop=0.0, use_rope=False, rel_bias=None):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.h = num_heads\n",
    "        self.dh = d_model // num_heads\n",
    "        # Use chunk later to separate: fewer kernel launches, better cache locality, and lower Python/autograd overhead.\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=True)\n",
    "        self.o   = nn.Linear(d_model, d_model, bias=True)\n",
    "        self.drop_p = p_drop\n",
    "        self.use_rope = use_rope\n",
    "        self.rel_bias = rel_bias  # nn.Module or None\n",
    "\n",
    "    def split_heads(self, x):  # [B,T,D] -> [B,H,T,Dh]\n",
    "        B,T,D = x.shape\n",
    "        x = x.view(B,T,self.h,self.dh).permute(0,2,1,3) # [B,T,D] -> [B, T, H, Dh] -> [B, H, T, Dh]\n",
    "        return x\n",
    "    def merge_heads(self, x):  # [B,H,T,Dh] -> [B,T,D]\n",
    "        B,H,T,Dh = x.shape\n",
    "        return x.permute(0,2,1,3).reshape(B,T,H*Dh)\n",
    "        # permute usually makes the tensor non-contiguous, .reshape() is equivalent to .contiguous().view()\n",
    "\n",
    "    def forward(self, x, pad_mask=None, causal=False):\n",
    "        # pad_mask: [B,T] bool, True=real token\n",
    "        B,T,D = x.shape\n",
    "        q,k,v = self.qkv(x).chunk(3, dim=-1)               # [B,T,D] each\n",
    "        q,k,v = self.split_heads(q), self.split_heads(k), self.split_heads(v)  # [B,H,T,Dh]\n",
    "\n",
    "        if self.use_rope:\n",
    "            q, k = apply_rope(q, k)                        # rotate Q/K in-place\n",
    "\n",
    "        # ---- Build SDPA attn_mask ----\n",
    "        # SDPA accepts:\n",
    "        #  * boolean mask: True = ALLOW attention (opposite of nn.MultiheadAttention)\n",
    "        #  * float mask: added to logits\n",
    "        \n",
    "        # Right before calling the encoder, once per batch:\n",
    "        assert pad_mask is None or (pad_mask.sum(dim=1) > 0).all(), \"Empty sequence in batch; add CLS/UNK or drop it.\"\n",
    "\n",
    "        if self.rel_bias is not None:\n",
    "            bias = self.rel_bias(T, device=x.device)       # [H,T,T]\n",
    "        attn_mask = build_sdpa_mask(pad_mask, bias, B=B, T=T, H=self.h, dtype=q.dtype, device=q.device)\n",
    "\n",
    "        for name, t in {\"q\": q, \"k\": k, \"v\": v}.items():\n",
    "            if not torch.isfinite(t).all():\n",
    "                raise RuntimeError(f\"{name} has non-finite values\")\n",
    "\n",
    "        if attn_mask is not None and attn_mask.dtype.is_floating_point:\n",
    "            if not torch.isfinite(attn_mask[attn_mask > -1e30]).all():  # ignore our -inf sentinels\n",
    "                raise RuntimeError(\"attn_mask has non-finite (non -inf) values\")\n",
    "            \n",
    "        # ---- SDPA (handles scale, softmax, dropout, matmul) ----\n",
    "        # SDPA expects [B,H,T,Dh]; attn_mask broadcastable to [B,H,T,T]\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=(self.drop_p if self.training else 0.0),\n",
    "            is_causal=bool(causal)\n",
    "        )                                                  # [B,H,T,Dh]\n",
    "        x = self.merge_heads(out)                          # [B,T,D]\n",
    "        x = self.o(x)                     # [B,T,D]\n",
    "        if pad_mask is not None:\n",
    "            x = x * pad_mask.unsqueeze(-1).to(x.dtype)  # zero padded query rows\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9aee0",
   "metadata": {},
   "source": [
    "# Plug SDPA MHA into Pre-LN encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc89d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLNEncoderBlockSDPA(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN Encoder block that *composes* your SDPAMHA attention module.\n",
    "    Pre-LN + residual wiring + FFN\n",
    "    x -> x + Drop( Attn( LN(x) ) )\n",
    "       -> x + Drop( FFN( LN(x) ) )\n",
    "\n",
    "    Expects:\n",
    "      - attn: a module like SDPAMHA with signature:\n",
    "              attn(x: [B,T,D], pad_mask: Optional[Bool[B,T]], causal: bool) -> [B,T,D]\n",
    "      - pad_mask: Bool[B,T], True = real token (not PAD)\n",
    "      - causal: usually False for encoders\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        *,\n",
    "        attn: nn.Module,          # <-- pass your SDPAMHA instance here\n",
    "        ff_mult: int = 4,\n",
    "        p_drop: float = 0.1,\n",
    "        norm: str = \"ln\",\n",
    "        resid_mode: str = \"plain\",  # {\"plain\",\"scaled\",\"rezero\"}\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = attn                       # <-- your SDPAMHA\n",
    "        self.ln1  = make_norm(norm, d_model)\n",
    "        self.drop1 = nn.Dropout(p_drop)\n",
    "\n",
    "        self.ln2  = make_norm(norm, d_model)\n",
    "        self.ff   = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_mult * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_mult * d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(p_drop)\n",
    "\n",
    "        self.mode = resid_mode\n",
    "        if resid_mode == \"rezero\":\n",
    "            self.g = nn.Parameter(torch.zeros(1))  # learnable gate\n",
    "        elif resid_mode == \"scaled\":\n",
    "            self.alpha = 0.5                       # constant residual scale\n",
    "\n",
    "    def _resid(self, x, h):\n",
    "        # residual add with optional scaling/gating\n",
    "        if self.mode == \"plain\":\n",
    "            return x + h\n",
    "        elif self.mode == \"scaled\":\n",
    "            return x + self.alpha * h\n",
    "        elif self.mode == \"rezero\":\n",
    "            return x + self.g * h\n",
    "        else:\n",
    "            raise ValueError(f\"unknown resid_mode={self.mode}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, pad_mask: torch.Tensor | None = None, causal: bool = False):\n",
    "        \"\"\"\n",
    "        x:        [B,T,D]\n",
    "        pad_mask: Bool[B,T], True = real token (not PAD). Will be passed through to SDPAMHA.\n",
    "        causal:   usually False for encoders\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        if pad_mask is not None:\n",
    "            assert pad_mask.dtype == torch.bool and pad_mask.shape == (B, T), \"pad_mask must be Bool[B,T]\"\n",
    "\n",
    "        # --- Attention branch (Pre-LN) ---\n",
    "        a_in = self.ln1(x)\n",
    "        a_out = self.attn(a_in, pad_mask=pad_mask, causal=causal)  # expects [B,T,D] from your SDPAMHA\n",
    "        x = self._resid(x, self.drop1(a_out))\n",
    "\n",
    "        # --- FFN branch (Pre-LN) ---\n",
    "        f_in = self.ln2(x)\n",
    "        f_out = self.ff(f_in)\n",
    "        x = self._resid(x, self.drop2(f_out))\n",
    "\n",
    "        return x  # [B,T,D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f00c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.text_helpers import make_tensors\n",
    "from src.encoder_classifier_wrapper import EncoderClassifier\n",
    "from src.train_utils import TrainConfig, kfold_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4278bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0）Prepare X, y, M (ids [B,T], labels [B], mask [B,T] bool)\n",
    "greeting_hard = [\n",
    "    \"good morning everyone\",\n",
    "    \"hello there my friend\",\n",
    "    \"hey buddy how are you\",\n",
    "    \"good evening folks\",\n",
    "    \"salutations from the sushi bar\",          # greeting + food word\n",
    "    \"pizza party greetings to all\",            # greeting + food word\n",
    "    \"hi from the ramen shop\",                  # greeting + food word\n",
    "    \"hello and welcome to brunch\",             # greeting + food word\n",
    "]\n",
    "food_hard = [\n",
    "    \"i love pizza\",\n",
    "    \"pasta is tasty tonight\",\n",
    "    \"fresh salad with apple\",\n",
    "    \"i like sushi a lot\",\n",
    "    \"good sandwich this morning\",              # food + greeting words\n",
    "    \"ramen is great hello world\",              # food + greeting word\n",
    "    \"eating an apple for breakfast\",\n",
    "    \"not a fan of pizza anymore\",              # negation\n",
    "]\n",
    "HARD_SUP = greeting_hard + food_hard\n",
    "HARD_LABELS = [0]*len(greeting_hard) + [1]*len(food_hard)\n",
    "X, M, y_float, y_long, stoi, itos, pad_id, cls_id = make_tensors(HARD_SUP, HARD_LABELS, min_freq=1, add_cls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54f92537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7963cc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_id, cls_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "456851ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build your attention block\n",
    "def make_block(d_model):\n",
    "    attn = SDPAMHA(d_model=d_model, num_heads=4, p_drop=0.1, use_rope=False, rel_bias=ClippedRelPosBias(4))\n",
    "    # ClippedRelPosBias(4)\n",
    "    return PreLNEncoderBlockSDPA(d_model, attn=attn, ff_mult=4, p_drop=0.1, norm=\"ln\", resid_mode=\"plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da46184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build the model ctor\n",
    "def make_model(vocab_size, d_model, pad_id, cls_id, num_layers=2, pool=\"mean\"):\n",
    "    return EncoderClassifier(\n",
    "        vocab_size=vocab_size, d_model=d_model, pad_id=pad_id,\n",
    "        num_layers=num_layers, block_ctor=make_block, pool=pool,\n",
    "        cls_id=cls_id, posenc=None, final_norm=\"ln\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a32718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47b6278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderClassifier(\n",
       "  (embed): Embedding(57, 64, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x PreLNEncoderBlockSDPA(\n",
       "      (attn): SDPAMHA(\n",
       "        (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (o): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (rel_bias): ClippedRelPosBias()\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "d_model = 64\n",
    "make_model(vocab_size, d_model, pad_id, cls_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cab1cc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16, 6]), torch.Size([1, 16, 6]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.unsqueeze(0).shape, M.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b1e09fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "X_ids = embed(X)\n",
    "X_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c24f166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4653e-01, -4.6931e-03,  1.6437e-01,  ..., -1.9432e-01,\n",
       "           1.5920e-01, -2.3063e-01],\n",
       "         [-1.6405e-01,  7.5195e-02,  1.6117e-01,  ..., -1.0296e-01,\n",
       "           1.9788e-01, -1.0972e-01],\n",
       "         [-1.0968e-01,  5.3158e-02,  1.1645e-01,  ..., -1.6102e-01,\n",
       "           9.6796e-02, -1.5366e-01],\n",
       "         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        [[-5.7306e-02, -1.1439e-01,  1.5806e-01,  ..., -2.3764e-01,\n",
       "           2.2892e-01, -2.1861e-01],\n",
       "         [-1.2536e-01, -8.5938e-02,  1.9407e-01,  ..., -7.1285e-02,\n",
       "           1.7287e-01, -1.4739e-01],\n",
       "         [-7.8699e-02, -1.6545e-01,  1.4682e-01,  ..., -1.4262e-01,\n",
       "           1.6964e-01, -5.7005e-02],\n",
       "         [-5.1923e-02, -1.2090e-01,  3.1135e-01,  ..., -2.1030e-01,\n",
       "           1.7257e-01, -1.5124e-01],\n",
       "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00],\n",
       "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        [[-1.1043e-01,  1.8153e-01,  2.8785e-01,  ...,  4.5079e-02,\n",
       "           5.2210e-02,  7.9058e-02],\n",
       "         [-8.2550e-02,  1.2452e-02,  2.9839e-01,  ...,  1.0157e-01,\n",
       "           9.0550e-02,  1.0783e-01],\n",
       "         [-1.5037e-01, -7.8167e-02,  2.5404e-01,  ...,  1.2423e-01,\n",
       "          -2.9127e-02, -5.1762e-03],\n",
       "         [-3.9972e-02, -4.7391e-03,  2.1326e-01,  ...,  1.9079e-01,\n",
       "          -8.2024e-02,  1.1274e-01],\n",
       "         [-6.8276e-02, -6.0208e-02,  2.2630e-01,  ...,  1.1089e-01,\n",
       "          -7.8551e-02,  3.9429e-02],\n",
       "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.9044e-01,  1.0235e-01,  2.0767e-01,  ..., -1.3072e-01,\n",
       "           2.5869e-02, -2.1288e-02],\n",
       "         [ 4.6316e-02, -1.5850e-01,  3.1351e-01,  ..., -9.9834e-02,\n",
       "           8.0980e-02, -8.0957e-02],\n",
       "         [ 4.9688e-02, -7.9268e-02,  1.8520e-01,  ..., -9.9197e-02,\n",
       "           7.8958e-02, -7.6605e-02],\n",
       "         [ 3.9168e-02,  1.9080e-02,  2.0082e-01,  ...,  1.0838e-03,\n",
       "           1.3519e-02,  1.3741e-02],\n",
       "         [ 1.4068e-01, -8.3613e-03,  2.1044e-01,  ..., -1.4557e-01,\n",
       "           1.4756e-01,  8.6728e-03],\n",
       "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        [[-2.4868e-01,  8.8127e-02, -4.0884e-04,  ..., -2.3785e-01,\n",
       "           1.9328e-01, -1.8039e-01],\n",
       "         [-2.3673e-01,  1.5691e-01, -5.8915e-02,  ..., -8.1923e-02,\n",
       "           2.9240e-01, -2.1719e-01],\n",
       "         [-2.9636e-01, -6.1230e-02, -1.5242e-02,  ..., -2.7301e-01,\n",
       "           1.9311e-01, -1.3209e-01],\n",
       "         [-3.3331e-01,  3.4244e-02, -2.6509e-02,  ..., -2.3408e-01,\n",
       "           2.0024e-01, -2.5350e-01],\n",
       "         [-1.7173e-01,  8.0250e-02, -7.1392e-02,  ..., -9.1199e-02,\n",
       "           2.0593e-01, -2.7410e-01],\n",
       "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "        [[-1.4847e-01,  3.1774e-01,  1.0121e-01,  ..., -1.0197e-01,\n",
       "           1.5690e-01, -1.6314e-01],\n",
       "         [-2.0557e-02,  1.9613e-01,  1.9849e-01,  ..., -2.0921e-02,\n",
       "           2.1044e-02, -1.6889e-01],\n",
       "         [-7.6595e-02,  2.7598e-01,  1.6251e-01,  ..., -7.7630e-02,\n",
       "           8.2397e-02, -1.4369e-01],\n",
       "         [-2.8411e-02,  3.1964e-01,  1.7565e-01,  ..., -1.3403e-01,\n",
       "           1.7118e-01, -1.2968e-01],\n",
       "         [-1.1911e-01,  2.3312e-01,  1.1000e-01,  ..., -4.9441e-02,\n",
       "           1.1302e-01, -1.4184e-01],\n",
       "         [-8.3042e-02,  9.4630e-02,  2.2008e-01,  ..., -8.2130e-02,\n",
       "           2.9709e-04, -1.6405e-01]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_attn_base = SDPAMHA(d_model=d_model, num_heads=4, p_drop=0.1, use_rope=False, rel_bias=None)\n",
    "test_attn_base(X_ids, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0c209bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1767,  0.0518, -0.0660,  ..., -0.0311, -0.0624,  0.1031],\n",
       "         [ 0.1430,  0.0246, -0.0819,  ...,  0.0052, -0.0574,  0.1335],\n",
       "         [ 0.1132,  0.0096, -0.1232,  ..., -0.0095, -0.0666,  0.0756],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0106, -0.1400, -0.2422,  ..., -0.1927, -0.1355, -0.0282],\n",
       "         [ 0.0740, -0.1267, -0.1998,  ..., -0.1578, -0.0612, -0.0183],\n",
       "         [-0.0796, -0.1274, -0.1989,  ..., -0.1984, -0.1063, -0.0164],\n",
       "         [ 0.0234, -0.0495, -0.1561,  ..., -0.1623, -0.0674,  0.0167],\n",
       "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.2408,  0.1793,  0.2721,  ..., -0.3403,  0.1242, -0.0357],\n",
       "         [ 0.1675,  0.2320,  0.2757,  ..., -0.5905,  0.0434,  0.0443],\n",
       "         [ 0.2027,  0.1579,  0.1768,  ..., -0.5184,  0.1069,  0.0164],\n",
       "         [ 0.2380,  0.1797,  0.2412,  ..., -0.4756,  0.1383, -0.1102],\n",
       "         [ 0.1998,  0.1763,  0.2366,  ..., -0.4813,  0.1125, -0.0711],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2775,  0.1502,  0.1572,  ..., -0.2845,  0.0540,  0.0354],\n",
       "         [ 0.3325,  0.1558,  0.1657,  ..., -0.2800,  0.1785, -0.0623],\n",
       "         [ 0.2439,  0.1327,  0.1345,  ..., -0.1405, -0.0069, -0.1677],\n",
       "         [ 0.3338,  0.2164,  0.2226,  ..., -0.2260,  0.0963, -0.0467],\n",
       "         [ 0.2851,  0.1441,  0.2518,  ..., -0.3027, -0.1006,  0.0319],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[ 0.0721,  0.1145, -0.1327,  ..., -0.2097,  0.0223,  0.1297],\n",
       "         [ 0.1078,  0.0571, -0.1998,  ..., -0.1311,  0.0150,  0.0777],\n",
       "         [ 0.0327,  0.2497, -0.2089,  ..., -0.0841,  0.0768,  0.1312],\n",
       "         [ 0.0590,  0.1197, -0.0443,  ..., -0.1307,  0.0486,  0.3190],\n",
       "         [ 0.0708,  0.0470, -0.3026,  ..., -0.0949, -0.0510,  0.0141],\n",
       "         [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1723,  0.1460, -0.1335,  ..., -0.1234, -0.1920,  0.0615],\n",
       "         [ 0.1965,  0.0974, -0.0778,  ..., -0.1963, -0.1063,  0.1724],\n",
       "         [ 0.1612, -0.0413, -0.2213,  ..., -0.1383, -0.2102,  0.2009],\n",
       "         [ 0.2717,  0.0324, -0.0285,  ..., -0.1125, -0.2264,  0.0134],\n",
       "         [ 0.1312,  0.0111, -0.0188,  ..., -0.1299, -0.0280,  0.0028],\n",
       "         [ 0.1680,  0.0289, -0.0782,  ..., -0.1971, -0.1861,  0.2211]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_attn = SDPAMHA(d_model=d_model, num_heads=4, p_drop=0.1, use_rope=False, rel_bias=ClippedRelPosBias(4))\n",
    "test_attn(X_ids, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce17a4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5011, -1.9598,  0.6901,  ...,  1.0972, -0.1493,  0.5156],\n",
       "         [ 0.0743,  0.6415, -0.0294,  ..., -0.0042, -0.0296, -0.4958],\n",
       "         [-1.5815, -0.0377, -0.7621,  ...,  1.4959, -1.3784,  1.8664],\n",
       "         [-0.0355,  0.1103,  0.0000,  ...,  0.0940, -0.0535,  0.0348],\n",
       "         [-0.0355,  0.1103, -0.0527,  ...,  0.0940, -0.0535,  0.0348],\n",
       "         [ 0.0000,  0.1103, -0.0527,  ...,  0.0940, -0.0535,  0.0348]],\n",
       "\n",
       "        [[ 0.0631,  0.2130, -0.2740,  ...,  0.4496,  0.7326,  0.6123],\n",
       "         [-0.5080,  0.6649,  1.0902,  ...,  0.0089,  3.0859, -1.7359],\n",
       "         [-0.4412,  1.6686, -0.5772,  ...,  1.1268, -1.9022, -1.3289],\n",
       "         [ 1.6179,  0.8584, -2.0297,  ...,  1.4784, -0.6282,  0.7068],\n",
       "         [-0.0355,  0.1103, -0.0527,  ...,  0.0940, -0.0535,  0.0348],\n",
       "         [-0.0355,  0.1103, -0.0527,  ...,  0.0940,  0.0000,  0.0348]],\n",
       "\n",
       "        [[ 1.6170,  0.5406,  0.9320,  ...,  0.6168, -1.4465, -1.2652],\n",
       "         [ 1.0966, -2.3336, -0.4699,  ...,  0.7605, -1.7515,  0.2239],\n",
       "         [ 0.9816,  0.5703, -0.2926,  ...,  2.3019, -0.3727,  0.2079],\n",
       "         [-1.3693,  1.4519, -1.1198,  ...,  0.2740, -1.3214,  0.7432],\n",
       "         [ 1.5454,  0.2008, -0.0215,  ...,  1.3463,  1.3057, -0.9015],\n",
       "         [-0.0355,  0.0000, -0.0527,  ...,  0.0940, -0.0535,  0.0348]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0481,  0.6971,  1.0203,  ...,  0.9508, -0.5093, -1.7575],\n",
       "         [ 0.0695, -0.3902,  1.0483,  ..., -0.3797, -0.4100, -0.9256],\n",
       "         [ 0.3854,  0.3852,  0.9329,  ...,  0.9102,  0.6676, -1.9340],\n",
       "         [-0.0630, -0.5126, -0.2483,  ...,  0.0498,  1.0703,  0.4204],\n",
       "         [ 0.4635, -0.8949,  0.9880,  ...,  0.5819,  0.8967,  1.0962],\n",
       "         [-0.0355,  0.1103, -0.0527,  ...,  0.0940, -0.0535,  0.0000]],\n",
       "\n",
       "        [[ 1.1895, -0.7678,  0.1688,  ...,  1.2158, -0.2419,  0.3602],\n",
       "         [-1.0977, -0.5579, -1.0410,  ...,  1.5462, -0.0961, -0.4629],\n",
       "         [ 0.0495, -1.4685, -1.4669,  ...,  0.3493, -0.6378,  1.8314],\n",
       "         [ 0.9480,  0.0833, -0.5324,  ...,  0.2813, -0.8890, -0.3907],\n",
       "         [ 0.3651, -0.7402, -2.2891,  ..., -0.2635, -0.3148, -1.2758],\n",
       "         [ 0.0000,  0.1103, -0.0527,  ...,  0.0940, -0.0535,  0.0348]],\n",
       "\n",
       "        [[-1.0339,  0.6271, -1.6539,  ..., -0.1925,  1.2519,  0.2826],\n",
       "         [-1.0961, -0.9379, -0.4400,  ..., -0.2642, -0.1088,  0.7971],\n",
       "         [-0.5285,  0.5178,  1.3112,  ...,  0.7574, -0.3688, -0.9169],\n",
       "         [-0.8068, -0.3485, -1.6205,  ..., -0.8456, -1.3853,  1.3848],\n",
       "         [-0.8622,  0.9349, -1.3401,  ...,  0.7800, -0.8041,  0.9216],\n",
       "         [ 1.2283, -0.1317, -0.3281,  ...,  1.3746,  0.0512, -1.1442]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_block = make_block(d_model)\n",
    "dummy_block(X_ids, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7546c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.9639574289321899', 'acc: 0.75', 'brier: 0.2648617923259735', 'margin: 0.3444175720214844']\n",
      "Fold 2, epoch 5, ['loss: 1.3912625312805176', 'acc: 0.3333333432674408', 'brier: 0.5033624172210693', 'margin: 0.33863815665245056']\n",
      "Fold 3, epoch 5, ['loss: 0.9259632229804993', 'acc: 0.6666666865348816', 'brier: 0.29952916502952576', 'margin: 0.32900500297546387']\n",
      "Fold 3, epoch 10, ['loss: 0.48906663060188293', 'acc: 0.6666666865348816', 'brier: 0.18294931948184967', 'margin: 0.35060420632362366']\n",
      "Fold 3, epoch 15, ['loss: 0.1283070147037506', 'acc: 1.0', 'brier: 0.021939540281891823', 'margin: 0.3844712972640991']\n",
      "Fold 3, epoch 20, ['loss: 0.040471915155649185', 'acc: 1.0', 'brier: 0.0029690610244870186', 'margin: 0.4611048698425293']\n",
      "Fold 3, epoch 25, ['loss: 0.033970560878515244', 'acc: 1.0', 'brier: 0.002316561061888933', 'margin: 0.46725329756736755']\n",
      "Fold 4, epoch 5, ['loss: 1.4336987733840942', 'acc: 0.6666666865348816', 'brier: 0.3618767261505127', 'margin: 0.3310765027999878']\n",
      "Fold 5, epoch 5, ['loss: 0.7260419726371765', 'acc: 0.6666666865348816', 'brier: 0.2565940022468567', 'margin: 0.29181432723999023']\n",
      "Fold 5, epoch 10, ['loss: 0.5948971509933472', 'acc: 0.6666666865348816', 'brier: 0.22489933669567108', 'margin: 0.41600021719932556']\n"
     ]
    }
   ],
   "source": [
    "# 3) Train with k-fold\n",
    "cfg = TrainConfig(epochs=40, batch_size=None, lr_enc=3e-3, warmup_steps=0)\n",
    "result = kfold_train(X, y_float, y_long, M, lambda: make_model(vocab_size, d_model, pad_id, cls_id), cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6407ef4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.6579810380935669,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.23283793032169342,\n",
       "   'margin': 0.09363312274217606},\n",
       "  {'loss': 0.819510281085968,\n",
       "   'acc': 0.0,\n",
       "   'brier': 0.31238171458244324,\n",
       "   'margin': 0.057350825518369675},\n",
       "  {'loss': 0.030911490321159363,\n",
       "   'acc': 1.0,\n",
       "   'brier': 0.0017949133180081844,\n",
       "   'margin': 0.47002896666526794},\n",
       "  {'loss': 0.6589000821113586,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.2330356389284134,\n",
       "   'margin': 0.04276571795344353},\n",
       "  {'loss': 0.34120258688926697,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.10634505748748779,\n",
       "   'margin': 0.26382431387901306}],\n",
       " 'mean': {'loss': 0.501701095700264,\n",
       "  'acc': 0.5666666746139526,\n",
       "  'brier': 0.1772790509276092,\n",
       "  'margin': 0.18552058935165405},\n",
       " 'std': {'loss': 0.28197171627743844,\n",
       "  'acc': 0.32659863480444,\n",
       "  'brier': 0.10979492692251096,\n",
       "  'margin': 0.16268142993309942}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
