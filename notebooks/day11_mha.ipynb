{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e868bdba",
   "metadata": {},
   "source": [
    "# Pre-LN manual MHA encoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37cd1cc",
   "metadata": {},
   "source": [
    "Supports (a) a padding mask (to ignore PAD tokens) and (b) an optional causal mask (to block attending to future positions). <br>\n",
    " - Encoder self-attention: causal=False, use padding mask. <br>\n",
    " - Decoder self-attention: causal=True (+ padding mask if sequences are padded). <br>\n",
    " - Cross-attention (decoder→encoder): causal=False, use encoder’s padding mask on K/V. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d19a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540e3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc10f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Masks ----------\n",
    "def make_padding_mask(lengths, T):\n",
    "    \"\"\"\n",
    "    lengths: LongTensor [B] of true lengths\n",
    "    returns mask [B, T] with True for real tokens, False for PAD\n",
    "    \"\"\"\n",
    "    rng = torch.arange(T, device=lengths.device).unsqueeze(0)  # [1,T]\n",
    "    return (rng < lengths.unsqueeze(1))                        # [B,T] bool\n",
    "\n",
    "def make_causal_mask(T, device, allow_k_to_q=False):\n",
    "    \"\"\"\n",
    "    returns [T,T] with True where attention is allowed.\n",
    "    Standard causal: each query t can attend to keys <= t.\n",
    "    \"\"\"\n",
    "    i = torch.arange(T, device=device).unsqueeze(1)\n",
    "    j = torch.arange(T, device=device).unsqueeze(0)\n",
    "    if allow_k_to_q:\n",
    "        # (not needed today) alternative patterns for enc-dec\n",
    "        pass\n",
    "    return (j <= i)  # True on/left of diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cfb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Manual Multi-Head Self-Attention ----------\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, p_drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" # num_heads of emebeddings concatenate to be d_model\n",
    "        self.d_model = d_model\n",
    "        self.h = num_heads\n",
    "        self.dh = d_model // num_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.scale = math.sqrt(self.dh)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B,T,D] -> [B,H,T,Dh]\n",
    "        B, T, D = x.shape\n",
    "        x = x.view(B, T, self.h, self.dh).permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        # x: [B,H,T,Dh] -> [B,T,D]\n",
    "        B, H, T, Dh = x.shape\n",
    "        # permute: [B, H, T, Dh] → [B, T, H, Dh]\n",
    "        # contiguous: ensure memory is laid out, materializes a copy in the new order (after permute, which is non-contiguous)\n",
    "        # view: → [B, T, D] where D = H*Dh. view can only reshape contiguous tensors\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(B, T, H * Dh)\n",
    "\n",
    "    def forward(self, x, pad_mask=None, causal=False):\n",
    "        \"\"\"\n",
    "        x: [B,T,D]; pad_mask: [B,T] (True=real, False=PAD)\n",
    "        returns: out [B,T,D], attn [B,H,T,T]; full attention matrix per head [T, T] (row: query token, col: key token)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        Q = self.split_heads(self.Wq(x))  # [B,H,T,Dh]\n",
    "        K = self.split_heads(self.Wk(x))  # [B,H,T,Dh]\n",
    "        V = self.split_heads(self.Wv(x))  # [B,H,T,Dh]\n",
    "\n",
    "        # scores: [B,H,T,T]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # padding mask (mask keys dimension): broadcast to [B,1,1,T]\n",
    "        if pad_mask is not None:\n",
    "            key_mask = pad_mask.unsqueeze(1).unsqueeze(1)  # True=keep\n",
    "            # mask keys before softmax so every query distributes probability only over valid keys\n",
    "            scores = scores.masked_fill(~key_mask, float(\"-inf\"))\n",
    "\n",
    "        # causal mask (prevent looking right): [T,T] -> [1,1,T,T]\n",
    "        if causal:\n",
    "            cm = make_causal_mask(T, x.device)  # True=keep\n",
    "            scores = scores.masked_fill(~cm.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)             # [B,H,T,T]\n",
    "        attn = self.dropout(attn)\n",
    "        # ctx = context (the attended representation), i.e., attn @ V\n",
    "        ctx = torch.matmul(attn, V)                      # [B,H,T,Dh]\n",
    "        out = self.Wo(self.merge_heads(ctx))             # [B,T,D]\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Pre-LN Encoder Block with MHA ----------\n",
    "class PreLNEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, p_drop: float = 0.1, ff_mult: int = 4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads, p_drop=p_drop)\n",
    "        self.drop1 = nn.Dropout(p_drop)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        # FFN needs enough channels to re-combine features nonlinearly per token; generally FFN hidden dim ≈ 4 × d_model\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_mult*d_model),\n",
    "            nn.GELU(), # Gaussian Error Linear Unit\n",
    "            nn.Linear(ff_mult*d_model, d_model),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x, pad_mask=None, causal=False):\n",
    "        a, attn = self.mha(self.ln1(x), pad_mask=pad_mask, causal=causal)\n",
    "        x = x + self.drop1(a)              # residual after MHA\n",
    "        f = self.ffn(self.ln2(x))\n",
    "        x = x + self.drop2(f)              # residual after FFN\n",
    "        return x, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738232c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B, T, D, H = 2, 6, 24, 4\n",
    "lengths = torch.tensor([6, 4])                # second sequence has 2 PADs\n",
    "pad_mask = make_padding_mask(lengths, T).to(DEVICE)  # [B,T]\n",
    "x = torch.randn(B, T, D, device=DEVICE)\n",
    "\n",
    "block = PreLNEncoderBlock(d_model=D, num_heads=H, p_drop=0.0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_pad shape: (2, 4, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "# Padding mask only (encoder usage)\n",
    "y_pad, attn_pad = block(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"attn_pad shape:\", tuple(attn_pad.shape))  # [B,H,T,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfa4ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn to PAD keys (batch 1, head 0): [0.3118632733821869, 0.24609842896461487, 0.20189827680587769, 0.24014002084732056, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Check PAD keys (last 2 positions in batch 2) get ~0 prob:\n",
    "print(\"attn to PAD keys (batch 1, head 0):\", attn_pad[1,0,-1].tolist())  # row: last query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72d9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_pad shape: (2, 8, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "# Try H = 8\n",
    "H = 8\n",
    "block = PreLNEncoderBlock(d_model=D, num_heads=H, p_drop=0.0).to(DEVICE)\n",
    "y_pad8, attn_pad8 = block(x, pad_mask=pad_mask, causal=False)\n",
    "print(\"attn_pad shape:\", tuple(attn_pad8.shape))  # [B,H,T,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd58236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal row (no pad mask) (batch 1, head 0): [0.1813250333070755, 0.1686900109052658, 0.16043618321418762, 0.076955147087574, 0.15375010669231415, 0.2588435411453247]\n"
     ]
    }
   ],
   "source": [
    "# Causal mask only (decoder usage)\n",
    "y_pad8_d, attn_pad8_d = block(x, pad_mask=None, causal=True)\n",
    "print(\"causal row (no pad mask) (batch 1, head 0):\", attn_pad8_d[1,0,-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25024086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal row t=2 (head 0): [0.31768906116485596, 0.6823108792304993, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Ensure future is masked: weights beyond diagonal ~0\n",
    "print(\"causal row t=1 (head 0):\", attn_pad8_d[0,0,1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f9d31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran with both masks (batch 1, head 0): [0.30868756771087646, 0.2871776819229126, 0.27312639355659485, 0.1310083568096161, 0.0, 0.0]\n",
      "shapes: (2, 6, 24)\n"
     ]
    }
   ],
   "source": [
    "# Both masks together (causal LM on padded batch)\n",
    "y_pad8_b, attn_pad8_b = block(x, pad_mask=pad_mask, causal=True)\n",
    "print(\"Ran with both masks (batch 1, head 0):\", attn_pad8_b[1,0,-1].tolist())\n",
    "print(\"shapes:\", tuple(y_pad8_b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.nn.MultiheadAttention ok: (2, 6, 24) (2, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare with PyTorch nn.MultiheadAttention shapes\n",
    "mha = nn.MultiheadAttention(embed_dim=D, num_heads=H, batch_first=True).to(DEVICE)\n",
    "attn_out, attn_w = mha(query=x, key=x, value=x, \n",
    "                       key_padding_mask=~pad_mask, need_weights=True, attn_mask=None, \n",
    "                       average_attn_weights=True, is_causal=False) \n",
    "print(\"torch.nn.MultiheadAttention ok:\", tuple(attn_out.shape), tuple(attn_w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09e3ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23362691700458527,\n",
       " 0.23248961567878723,\n",
       " 0.2443876713514328,\n",
       " 0.2894957959651947,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_w[1,0,:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96f87262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = make_causal_mask(T, x.device)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23cfe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True,  True]], device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e226255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.4493405818939209, 0.5506594181060791, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.35129815340042114, 0.3132854104042053, 0.33541643619537354, 0.0, 0.0, 0.0],\n",
       " [0.3033524453639984,\n",
       "  0.2528725266456604,\n",
       "  0.2039852887392044,\n",
       "  0.23978973925113678,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.27934426069259644,\n",
       "  0.2716120481491089,\n",
       "  0.21209318935871124,\n",
       "  0.23695051670074463,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.21622858941555023,\n",
       "  0.26489898562431335,\n",
       "  0.2565867304801941,\n",
       "  0.26228567957878113,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_out_d, attn_w_d = mha(query=x, key=x, value=x, \n",
    "                       key_padding_mask=~pad_mask, need_weights=True, attn_mask=~causal_mask, \n",
    "                       average_attn_weights=True, is_causal=False) \n",
    "attn_w_d[1,:,:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4770387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6, 6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add p_drop=0.1 and re-run to see effect.\n",
    "H = 4\n",
    "block_do = PreLNEncoderBlock(d_model=D, num_heads=H, p_drop=0.1).to(DEVICE)\n",
    "y_pad_do, attn_pad_do = block_do(x, pad_mask=pad_mask, causal=False)\n",
    "attn_pad_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec20630d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.26510390639305115,\n",
       "  0.25778496265411377,\n",
       "  0.285604864358902,\n",
       "  0.3026174008846283,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.30113857984542847, 0.0, 0.2674764096736908, 0.0, 0.0, 0.0],\n",
       " [0.3511961102485657, 0.0, 0.2110809087753296, 0.2361455112695694, 0.0, 0.0],\n",
       " [0.31595924496650696,\n",
       "  0.27965593338012695,\n",
       "  0.24938443303108215,\n",
       "  0.2661115527153015,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.27424779534339905,\n",
       "  0.26770955324172974,\n",
       "  0.2704418897628784,\n",
       "  0.29871201515197754,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.27505114674568176, 0.32703322172164917, 0.2993795871734619, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_pad_do[1,0].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
