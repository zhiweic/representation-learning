{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea992bd9",
   "metadata": {},
   "source": [
    "# Temporal Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102a0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba6886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3f8af",
   "metadata": {},
   "source": [
    "The Conv1d output length is L_out = [(L_in+2p-d(k-1)-1)/s]+1\n",
    "where k=kernel size, d=dilation, s=stride, p=padding on each side. <br>\n",
    "For our blocks, we want L_out = L_in, p = d(k-1)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e742b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Conv residual block for [B, T, D].\n",
    "    - depthwise separable optional (kept simple: standard conv here)\n",
    "    - GLU gating on the first conv\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, *, kernel_size=5, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d = d_model\n",
    "        self.ks = kernel_size\n",
    "        self.dil = dilation\n",
    "        pad = (kernel_size - 1) // 2 * dilation  # “same” length after a 1-D conv with dilation for non-causal\n",
    "\n",
    "        # GLU: we need two channel groups (one for content, one for gates)\n",
    "        self.conv1 = nn.Conv1d(d_model, 2 * d_model, kernel_size, padding=pad, dilation=dilation)\n",
    "        self.bn1   = nn.BatchNorm1d(2 * d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size, padding=pad, dilation=dilation)\n",
    "        self.bn2   = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        # residual scale to help very deep stacks\n",
    "        self.res_scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x, mask=None, **kwargs):   # ← accept & ignore extra kwarg,  x: [B, T, D]\n",
    "        h = x.transpose(1, 2)                 # -> [B, D, T]\n",
    "        y = self.conv1(h)                     # [B, 2D, T]\n",
    "        y = self.bn1(y)\n",
    "        a, b = y.chunk(2, dim=1)              # split channels, each [B,D,T]\n",
    "        y = a * torch.sigmoid(b)              # GLU\n",
    "        y = self.dropout(y)\n",
    "        y = self.conv2(y)                     # [B, D, T]\n",
    "        y = self.bn2(y)\n",
    "        y = y.transpose(1, 2)                 # -> [B, T, D]\n",
    "        return x + self.res_scale * F.gelu(y) # PreNorm isn’t needed; BN is in conv path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be9dae",
   "metadata": {},
   "source": [
    "Receptive field sanity (pick layers vs crop_len)\n",
    "\n",
    "For dilations 1,2,…,2^L−1 and kernel k, non-causal RF:\n",
    "\n",
    "RF=1+(k−1) (i=0~L−1)∑2^i = 1 + (k-1)*(2^L - 1) (non-causal, stride 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5f3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure RF roughly covers the crop\n",
    "def tcn_rf(k,L): return 1 + (k-1)*(2**L - 1)\n",
    "\n",
    "tcn_rf(5,4) # sweet spot for 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df706e",
   "metadata": {},
   "source": [
    "## Dilation schedule factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bec498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tcn_block_ctor(kernel_size=5, dropout=0.1, base=2):\n",
    "    \"\"\"\n",
    "    Returns a function block_ctor(d_model) that creates blocks with doubling dilation:\n",
    "    1, 2, 4, 8, ...\n",
    "    \"\"\"\n",
    "    counter = {\"i\": 0}\n",
    "    def block_ctor(d_model: int):\n",
    "        dil = base ** counter[\"i\"]\n",
    "        counter[\"i\"] += 1\n",
    "        return TCNResidualBlock(d_model, kernel_size=kernel_size, dilation=dil, dropout=dropout)\n",
    "    return block_ctor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "018e6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.encoder_classifier_wrapper import EncoderClassifier, LinearFrontend, SimCLRProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac8ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontend on [B,T,C] -> [B,T,D]\n",
    "C = 1\n",
    "frontend = LinearFrontend(in_channels=C, d_model=64)\n",
    "\n",
    "# dilations: 1,2,4,8 for num_layers=4 (RF grows fast; see note below)\n",
    "block_ctor = make_tcn_block_ctor(kernel_size=5, dropout=0.1, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8854e658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNResidualBlock(\n",
       "  (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ctor(d_model=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8942558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.synthetic_data import synth_trips\n",
    "from src.ts_contrastive import TwoCropTSDataset, TSView, nt_xent, info_nce_two_way, train_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f85f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, road, wthr, t = synth_trips(N=2000, T=128, use_accel=False, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "029c2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = TSView()\n",
    "ds = TwoCropTSDataset(X, M=None, view=view, crop_len=64)\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=256, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "411da64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector_bn = SimCLRProjector(d=64, p=64)\n",
    "model = EncoderClassifier(\n",
    "    d_model=64, num_layers=4, block_ctor=block_ctor,\n",
    "    pool=\"mean\", posenc=None,                     # posenc=None for conv stacks\n",
    "    final_norm=\"ln\", final_norm_pos=\"post_pool\",\n",
    "    proj_dim=64, frontend=frontend, projector=projector_bn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bad1bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001] loss=4.5752 | z-std=0.124 |intra diag/off=1.000/0.014 | cross diag/off=0.863/0.015\n",
      "[epoch 002] loss=4.3276 | z-std=0.124 |intra diag/off=1.000/0.012 | cross diag/off=0.868/0.014\n",
      "[epoch 003] loss=4.1402 | z-std=0.124 |intra diag/off=1.000/0.006 | cross diag/off=0.839/0.007\n",
      "[epoch 004] loss=4.0773 | z-std=0.124 |intra diag/off=1.000/0.006 | cross diag/off=0.853/0.006\n",
      "[epoch 005] loss=4.0979 | z-std=0.124 |intra diag/off=1.000/0.011 | cross diag/off=0.839/0.012\n",
      "[epoch 006] loss=3.9950 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.846/0.007\n",
      "[epoch 007] loss=3.9831 | z-std=0.124 |intra diag/off=1.000/0.005 | cross diag/off=0.845/0.006\n",
      "[epoch 008] loss=3.9632 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.832/0.008\n",
      "[epoch 009] loss=4.0133 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.822/0.005\n",
      "[epoch 010] loss=4.0192 | z-std=0.124 |intra diag/off=1.000/0.008 | cross diag/off=0.844/0.008\n",
      "[epoch 011] loss=3.9647 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.817/0.008\n",
      "[epoch 012] loss=3.8884 | z-std=0.125 |intra diag/off=1.000/0.000 | cross diag/off=0.836/0.001\n",
      "[epoch 013] loss=3.9262 | z-std=0.124 |intra diag/off=1.000/0.005 | cross diag/off=0.832/0.005\n",
      "[epoch 014] loss=3.8676 | z-std=0.124 |intra diag/off=1.000/0.003 | cross diag/off=0.823/0.004\n",
      "[epoch 015] loss=3.8621 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.824/0.007\n",
      "[epoch 016] loss=3.8688 | z-std=0.124 |intra diag/off=1.000/0.009 | cross diag/off=0.796/0.009\n",
      "[epoch 017] loss=3.8383 | z-std=0.124 |intra diag/off=1.000/0.006 | cross diag/off=0.818/0.006\n",
      "[epoch 018] loss=3.8431 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.820/0.004\n",
      "[epoch 019] loss=3.8167 | z-std=0.124 |intra diag/off=1.000/0.005 | cross diag/off=0.810/0.006\n",
      "[epoch 020] loss=3.8195 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.791/0.005\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_contrastive(model, loader, loss_fn=nt_xent, epochs=20, lr=1e-3, tau=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077dd6e4",
   "metadata": {},
   "source": [
    "InfoNCE often trades a bit of alignment (positive similarity) for uniformity (negatives pushed apart), which still lowers the loss—as your logs show (off-diag ≈ 0.004–0.015). What matters is a validation metric (e.g., frozen linear-probe accuracy or k-NN retrieval), not the raw loss or cross-diag alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99fa9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 001] loss=3.3340 | z-std=0.124 |intra diag/off=1.000/0.008 | cross diag/off=0.785/0.010\n",
      "[epoch 002] loss=3.2101 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.808/0.004\n",
      "[epoch 003] loss=3.2079 | z-std=0.124 |intra diag/off=1.000/0.003 | cross diag/off=0.815/0.004\n",
      "[epoch 004] loss=3.1524 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.800/0.008\n",
      "[epoch 005] loss=3.1588 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.806/0.005\n",
      "[epoch 006] loss=3.1943 | z-std=0.124 |intra diag/off=1.000/0.006 | cross diag/off=0.777/0.006\n",
      "[epoch 007] loss=3.1581 | z-std=0.124 |intra diag/off=1.000/0.008 | cross diag/off=0.802/0.009\n",
      "[epoch 008] loss=3.1363 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.801/0.005\n",
      "[epoch 009] loss=3.1954 | z-std=0.125 |intra diag/off=1.000/0.001 | cross diag/off=0.795/0.002\n",
      "[epoch 010] loss=3.1526 | z-std=0.124 |intra diag/off=1.000/0.002 | cross diag/off=0.795/0.003\n",
      "[epoch 011] loss=3.1083 | z-std=0.124 |intra diag/off=1.000/0.003 | cross diag/off=0.748/0.003\n",
      "[epoch 012] loss=3.1196 | z-std=0.124 |intra diag/off=1.000/0.007 | cross diag/off=0.808/0.007\n",
      "[epoch 013] loss=3.1042 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.779/0.005\n",
      "[epoch 014] loss=3.0897 | z-std=0.124 |intra diag/off=1.000/0.004 | cross diag/off=0.782/0.005\n",
      "[epoch 015] loss=3.0530 | z-std=0.125 |intra diag/off=1.000/0.001 | cross diag/off=0.764/0.002\n",
      "[epoch 016] loss=3.0442 | z-std=0.125 |intra diag/off=1.000/0.000 | cross diag/off=0.781/0.001\n",
      "[epoch 017] loss=2.9954 | z-std=0.124 |intra diag/off=1.000/0.003 | cross diag/off=0.769/0.003\n",
      "[epoch 018] loss=3.0134 | z-std=0.125 |intra diag/off=1.000/0.001 | cross diag/off=0.763/0.002\n",
      "[epoch 019] loss=3.0715 | z-std=0.125 |intra diag/off=1.000/-0.000 | cross diag/off=0.751/0.000\n",
      "[epoch 020] loss=3.0308 | z-std=0.125 |intra diag/off=1.000/-0.001 | cross diag/off=0.781/0.000\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_contrastive(model, loader, loss_fn=info_nce_two_way, epochs=20, lr=1e-3, tau=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1988b",
   "metadata": {},
   "source": [
    "### The two SimCLR-style losses\n",
    "\n",
    "NT-Xent (2B×2B)\n",
    "Concatenate 2 views, compute all pairwise similarities.\n",
    "Negatives: All other samples from both views ⇒ per anchor: 2B−2 negatives.\n",
    "More negative counts often yields stronger uniformity\n",
    "\n",
    "Two-way cross-view InfoNCE (B×B)\n",
    "Use only cross-view sims\n",
    "Negatives: Only cross-view ⇒ per anchor: B−1 negatives.\n",
    "B×B is simpler and more stable.\n",
    "\n",
    "Temperature tuning: With fewer negatives (B×B), you often need a smaller τ to keep the positive sharp. Switching from 2B→B×B frequently benefits from reducing τ (e.g., 0.2→0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a37d1",
   "metadata": {},
   "source": [
    "## Linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "177bb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.linear_probe import split_idx, train_linear_probe, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cacd3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f51dc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "enc = model.features  # or a wrapper that returns pooled pre-projection h\n",
    "with torch.no_grad():\n",
    "    Z = enc(X.to(dev))   # [N,D]\n",
    "# simple linear probes (logreg) for road (3) & weather (2)\n",
    "clf_road = torch.nn.Linear(Z.size(1), 3).to(dev)\n",
    "clf_wthr = torch.nn.Linear(Z.size(1), 2).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bff3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "train_idx, val_idx = split_idx(Z.size(0), val=0.2, seed=42)\n",
    "Ztr, Zva = Z[train_idx], Z[val_idx]\n",
    "road_tr, road_va = road[train_idx], road[val_idx]\n",
    "wthr_tr, wthr_va = wthr[train_idx], wthr[val_idx]\n",
    "\n",
    "# Train probes\n",
    "W_road = train_linear_probe(Ztr, road_tr, clf_road, epochs=200, lr=1e-2, wd=1e-4, device=device)\n",
    "W_wthr = train_linear_probe(Ztr, wthr_tr, clf_wthr, epochs=200, lr=1e-2, wd=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fd80e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear probe – road: 0.905, weather: 0.685\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "acc_road = accuracy(W_road, Zva, road_va, device=device)\n",
    "acc_wthr = accuracy(W_wthr, Zva, wthr_va, device=device)\n",
    "print(f\"Linear probe – road: {acc_road:.3f}, weather: {acc_wthr:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
