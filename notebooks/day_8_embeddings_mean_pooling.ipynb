{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3631e0f2",
   "metadata": {},
   "source": [
    "# Build a tiny pipeline: token IDs -> embeddings -> mean pooling -> sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b1e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754b9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "EMBED_DIM = 32      # small on purpose (try 16/64 later)\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58389898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny toy corpus + vocab\n",
    "TOY_SENTENCES = [\n",
    "    \"hello world\",\n",
    "    \"hello there\",\n",
    "    \"spam ham eggs\",\n",
    "    \"ham and eggs\",\n",
    "    \"representation learning is fun\",\n",
    "    \"embeddings with mean pooling\",\n",
    "    \"we love simple baselines\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c7c288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<unk>',\n",
       " 2: 'eggs',\n",
       " 3: 'ham',\n",
       " 4: 'hello',\n",
       " 5: 'and',\n",
       " 6: 'baselines',\n",
       " 7: 'embeddings',\n",
       " 8: 'fun',\n",
       " 9: 'is',\n",
       " 10: 'learning',\n",
       " 11: 'love',\n",
       " 12: 'mean',\n",
       " 13: 'pooling',\n",
       " 14: 'representation',\n",
       " 15: 'simple',\n",
       " 16: 'spam',\n",
       " 17: 'there',\n",
       " 18: 'we',\n",
       " 19: 'with',\n",
       " 20: 'world'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(sentences: List[str],\n",
    "                min_freq: int = 1) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a tiny word-level vocab: {token -> id}.\n",
    "    Includes PAD and UNK.\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for s in sentences:\n",
    "        for w in s.strip().split():\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    # sort by frequency (desc), then token (asc) for determinism\n",
    "    words = [w for w, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])) if c >= min_freq]\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for w in words:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "VOCAB = build_vocab(TOY_SENTENCES)\n",
    "ID2TOK = {i: t for t, i in VOCAB.items()}\n",
    "ID2TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2078c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization helpers\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    return sentence.strip().split()\n",
    "\n",
    "def numericalize(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n",
    "    return [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]\n",
    "\n",
    "def pad_batch(batch_ids: List[List[int]], pad_id: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pad to max length in batch.\n",
    "    Returns:\n",
    "        ids: LongTensor [B, T]\n",
    "        mask: BoolTensor [B, T] where True = valid token (not PAD)\n",
    "    \"\"\"\n",
    "    max_len = max(len(x) for x in batch_ids) if batch_ids else 0\n",
    "    padded = []\n",
    "    mask = []\n",
    "    for seq in batch_ids:\n",
    "        pad_len = max_len - len(seq)\n",
    "        padded.append(seq + [pad_id] * pad_len)\n",
    "        mask.append([1] * len(seq) + [0] * pad_len)\n",
    "    return torch.tensor(padded, dtype=torch.long), torch.tensor(mask, dtype=torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598d0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + Mean Pooling\n",
    "class MeanPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean-pool token embeddings over non-pad tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    [B, T, D] embeddings\n",
    "        mask: [B, T]    True for real tokens\n",
    "        returns:\n",
    "            sent_emb: [B, D]\n",
    "        \"\"\"\n",
    "        mask_f = mask.float().unsqueeze(-1)     # [B, T, 1]\n",
    "        summed = (x * mask_f).sum(dim=1)        # [B, D]\n",
    "        counts = mask_f.sum(dim=1).clamp(min=1) # [B, 1]\n",
    "        return summed / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a828dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbeddingModelConfig:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    pad_id: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebac0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal embedding -> mean pooling encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: EmbeddingModelConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim, padding_idx=cfg.pad_id)\n",
    "        self.pool = MeanPooler()\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # token_ids: [B, T]\n",
    "        emb = self.embedding(token_ids)         # [B, T, D]\n",
    "        sent_vec = self.pool(emb, mask)         # [B, D]\n",
    "        return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0730c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity helper\n",
    "def cosine_sim_matrix(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B, D]\n",
    "    returns:\n",
    "        cos_sim: [B, B] cosine similarity\n",
    "    \"\"\"\n",
    "    x = F.normalize(x, p=2, dim=-1)\n",
    "    return x @ x.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245041bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_mean_pooling(sentences: List[str]) -> None:\n",
    "    print(f\"Vocab size: {len(VOCAB)} | Embed dim: {EMBED_DIM}\")\n",
    "    print(f\"Vocab tokens: {list(VOCAB.keys())}\\n\")\n",
    "\n",
    "    # Build inputs\n",
    "    tokenized = [tokenize(s) for s in sentences]\n",
    "    ids = [numericalize(tok, VOCAB) for tok in tokenized]\n",
    "    ids_tensor, mask = pad_batch(ids, pad_id=VOCAB[PAD_TOKEN])\n",
    "\n",
    "    # Model\n",
    "    cfg = EmbeddingModelConfig(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=VOCAB[PAD_TOKEN])\n",
    "    model = TinyEmbeddingModel(cfg)\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        sent_vecs = model(ids_tensor, mask)   # [B, D]\n",
    "\n",
    "    # Show shapes\n",
    "    print(f\"Input IDs shape:     {ids_tensor.shape}  (B,T)\")\n",
    "    print(f\"Embeddings shape:    {[len(sentences), max(len(x) for x in ids)]} -> pooled to (B,D) = {sent_vecs.shape}\")\n",
    "\n",
    "    # Inspect a couple vectors\n",
    "    for i, s in enumerate(sentences[:3]):\n",
    "        print(f\"\\nSentence[{i}]: \\\"{s}\\\"\")\n",
    "        print(f\"Vector (first 8 dims): {sent_vecs[i, :8].tolist()}\")\n",
    "\n",
    "    # Optional: cosine similarity between all sentence vectors\n",
    "    cos = cosine_sim_matrix(sent_vecs)\n",
    "    print(\"\\nCosine similarity matrix (rounded to 3 dp):\")\n",
    "    with torch.no_grad():\n",
    "        print(torch.round(cos * 1000) / 1000)\n",
    "\n",
    "    # Map IDs back to tokens for the first batch item (debugging/learning)\n",
    "    print(\"\\nExample token reconstruction for sample[0]:\")\n",
    "    first_ids = ids_tensor[0].tolist()\n",
    "    print([ID2TOK[i] for i in first_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bccef39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 21 | Embed dim: 32\n",
      "Vocab tokens: ['<pad>', '<unk>', 'eggs', 'ham', 'hello', 'and', 'baselines', 'embeddings', 'fun', 'is', 'learning', 'love', 'mean', 'pooling', 'representation', 'simple', 'spam', 'there', 'we', 'with', 'world']\n",
      "\n",
      "Input IDs shape:     torch.Size([7, 4])  (B,T)\n",
      "Embeddings shape:    [7, 4] -> pooled to (B,D) = torch.Size([7, 32])\n",
      "\n",
      "Sentence[0]: \"hello world\"\n",
      "Vector (first 8 dims): [0.3262319564819336, 0.48671817779541016, 0.23869723081588745, -0.39600759744644165, -0.005489230155944824, 0.4366290867328644, -0.2679099440574646, 0.6758059859275818]\n",
      "\n",
      "Sentence[1]: \"hello there\"\n",
      "Vector (first 8 dims): [0.09677910804748535, 0.44325578212738037, -1.4011105298995972, -0.009056806564331055, -0.3794155716896057, 0.4231144189834595, 0.24345803260803223, 1.1096596717834473]\n",
      "\n",
      "Sentence[2]: \"spam ham eggs\"\n",
      "Vector (first 8 dims): [-0.8093934059143066, 0.5124570727348328, 0.7974427342414856, 0.2383490651845932, 0.8325245976448059, 0.19940675795078278, -0.23762141168117523, -0.04164385795593262]\n",
      "\n",
      "Cosine similarity matrix (rounded to 3 dp):\n",
      "tensor([[ 1.0000,  0.4830, -0.0020, -0.0890,  0.0470, -0.2140,  0.1310],\n",
      "        [ 0.4830,  1.0000, -0.1100, -0.3010,  0.0210, -0.1050,  0.0850],\n",
      "        [-0.0020, -0.1100,  1.0000,  0.6340,  0.1260,  0.0390, -0.0710],\n",
      "        [-0.0890, -0.3010,  0.6340,  1.0000, -0.1010,  0.1540,  0.1070],\n",
      "        [ 0.0470,  0.0210,  0.1260, -0.1010,  1.0000,  0.0480,  0.2300],\n",
      "        [-0.2140, -0.1050,  0.0390,  0.1540,  0.0480,  1.0000, -0.0090],\n",
      "        [ 0.1310,  0.0850, -0.0710,  0.1070,  0.2300, -0.0090,  1.0000]])\n",
      "\n",
      "Example token reconstruction for sample[0]:\n",
      "['hello', 'world', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "demo_mean_pooling(TOY_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45e0fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_one_hot_projection(sentences):\n",
    "    \"\"\"\n",
    "    - Create a one-hot representation for each token (size = vocab_size).\n",
    "    - Learn a linear projection W: [vocab_size -> EMBED_DIM].\n",
    "    - Show that one-hot @ W is mathematically equivalent to nn.Embedding (without padding handling).\n",
    "    Hints:\n",
    "      - Build an identity matrix I[vocab_size, vocab_size].\n",
    "      - Pick rows with token IDs -> gives one-hot rows. [B,T,V]\n",
    "      - W=model.embedding.weight[V,D]\n",
    "    \"\"\"\n",
    "    tokenized = [tokenize(s) for s in sentences]\n",
    "    ids = [numericalize(tok, VOCAB) for tok in tokenized]\n",
    "    ids_tensor, mask = pad_batch(ids, pad_id=VOCAB[PAD_TOKEN])\n",
    "    V, D, pad_id = len(VOCAB), EMBED_DIM, VOCAB[PAD_TOKEN]\n",
    "\n",
    "    # 1) Build an Embedding model and grab its weight matrix W_embed (V,D)\n",
    "    cfg = EmbeddingModelConfig(vocab_size=V, embed_dim=D, pad_id=pad_id)\n",
    "    emb_model = TinyEmbeddingModel(cfg)\n",
    "    W_embed = emb_model.embedding.weight.detach().clone()  # [V, D]\n",
    "    # detach(): stops autograd from tracking the matmul with one-hot. clone(): makes a snapshot copy.\n",
    "\n",
    "    # 2) Make one-hot for the same ids: (B,T,V)\n",
    "    identity_matrix = torch.eye(len(VOCAB))\n",
    "    one_hot = torch.stack([identity_matrix[ids_tensor[s],:] for s in range(len(ids_tensor))])\n",
    "    # one_hot = F.one_hot(ids_tensor, num_classes=len(VOCAB)).float()\n",
    "\n",
    "    # Project with W as a linear map: (B,T,V) @ (V,D) -> (B,T,D)\n",
    "    proj_vecs = torch.matmul(one_hot, W_embed)  # [B,T,D]\n",
    "    proj_vecs = proj_vecs.masked_fill(ids_tensor.eq(pad_id).unsqueeze(-1), 0.0)\n",
    "\n",
    "    # Compare with nn.Embedding lookup:\n",
    "    with torch.no_grad():\n",
    "        lookup_vecs = emb_model.embedding(ids_tensor)  # [B,T,D]\n",
    "\n",
    "\n",
    "\n",
    "    same = torch.allclose(proj_vecs, lookup_vecs, atol=1e-6)\n",
    "    print(f\"One-hot @ W equals Embedding lookup? {same}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8dbe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot @ W equals Embedding lookup? True\n"
     ]
    }
   ],
   "source": [
    "compare_one_hot_projection(TOY_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe41a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TOY_SENTENCES = [\n",
    "    \"eggs and ham\",\n",
    "    \"representation learning is not fun\",\n",
    "    \"we like simple baselines\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf3a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_tensor(sentences):\n",
    "    tokenized = [tokenize(s) for s in sentences]\n",
    "    ids = [numericalize(tok, VOCAB) for tok in tokenized]\n",
    "    ids_tensor, mask = pad_batch(ids, pad_id=VOCAB[PAD_TOKEN])\n",
    "    return ids_tensor, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c0d5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity_probe(sentences):\n",
    "    \"\"\"\n",
    "    - Add 2–3 new sentences that are paraphrases/near-duplicates of existing ones.\n",
    "    - Re-run embeddings + mean pooling and look at cosine similarities for those pairs.\n",
    "    - Reflect: mean pooling is order-invariant; what does that imply?\n",
    "    \"\"\"\n",
    "    ids_tensor, mask = get_padded_tensor(sentences)\n",
    "\n",
    "    # Model\n",
    "    cfg = EmbeddingModelConfig(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=VOCAB[PAD_TOKEN])\n",
    "    model = TinyEmbeddingModel(cfg)\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        sent_vecs = model(ids_tensor, mask)   # [B, D]\n",
    "\n",
    "    # Get cosine similarity between all sentence vectors\n",
    "    cos = cosine_sim_matrix(sent_vecs)\n",
    "    # Set diagnal to 0.0\n",
    "    cos.fill_diagonal_(0.)\n",
    "    farthest, closest = torch.argmin(cos, dim=1), torch.argmax(cos, dim=1)\n",
    "    for i in range(len(sentences)):\n",
    "        print(f\"Sentence: {sentences[i]} | Closest match: {sentences[closest[i]]} | Farthest: {sentences[farthest[i]]}\")\n",
    "    \n",
    "    return sent_vecs, cos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa68b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hello world | Closest match: hello there | Farthest: we like simple baselines\n",
      "Sentence: hello there | Closest match: hello world | Farthest: we like simple baselines\n",
      "Sentence: spam ham eggs | Closest match: ham and eggs | Farthest: we like simple baselines\n",
      "Sentence: ham and eggs | Closest match: eggs and ham | Farthest: embeddings with mean pooling\n",
      "Sentence: representation learning is fun | Closest match: representation learning is not fun | Farthest: we like simple baselines\n",
      "Sentence: embeddings with mean pooling | Closest match: we love simple baselines | Farthest: representation learning is not fun\n",
      "Sentence: we love simple baselines | Closest match: we like simple baselines | Farthest: hello world\n",
      "Sentence: eggs and ham | Closest match: ham and eggs | Farthest: embeddings with mean pooling\n",
      "Sentence: representation learning is not fun | Closest match: representation learning is fun | Farthest: embeddings with mean pooling\n",
      "Sentence: we like simple baselines | Closest match: we love simple baselines | Farthest: hello world\n"
     ]
    }
   ],
   "source": [
    "sent_vecs, cos = sentence_similarity_probe(TOY_SENTENCES+NEW_TOY_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54296712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st = SentenceTransformer('sentence-transformers/stsb-bert-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4c2e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91d8aad7b174107a3c6661d520cbb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st_vecs = st.encode(sentences=TOY_SENTENCES+NEW_TOY_SENTENCES, batch_size=64, convert_to_tensor=True,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7eaefca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent_vecs), type(st_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0e33f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cosine_similarity(sent_vecs):\n",
    "    # Get cosine similarity between all sentence vectors\n",
    "    cos = cosine_sim_matrix(sent_vecs)\n",
    "    # Set diagnal to 0.0\n",
    "    cos.fill_diagonal_(0.)\n",
    "    farthest, closest = torch.argmin(cos, dim=1), torch.argmax(cos, dim=1)\n",
    "    \n",
    "    return farthest, closest\n",
    "    \n",
    "\n",
    "def compare_two_embeddings(sentences, sent_vecs1, sent_vecs2):\n",
    "    farthest1, closest1 = compare_cosine_similarity(sent_vecs1)\n",
    "    farthest2, closest2 = compare_cosine_similarity(sent_vecs2)\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        print(f\"\"\"Sentence: {sentences[i]}\n",
    "              Embedding 1 Closest : {sentences[closest1[i]]} | Farthest: {sentences[farthest1[i]]}, \n",
    "              Embedding 2 Closest : {sentences[closest2[i]]} | Farthest: {sentences[farthest2[i]]}\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: hello world\n",
      "              Embedding 1 Closest : hello there | Farthest: we like simple baselines, \n",
      "              Embedding 2 Closest : hello there | Farthest: representation learning is not fun\n",
      "Sentence: hello there\n",
      "              Embedding 1 Closest : hello world | Farthest: we like simple baselines, \n",
      "              Embedding 2 Closest : hello world | Farthest: representation learning is not fun\n",
      "Sentence: spam ham eggs\n",
      "              Embedding 1 Closest : ham and eggs | Farthest: we like simple baselines, \n",
      "              Embedding 2 Closest : eggs and ham | Farthest: representation learning is fun\n",
      "Sentence: ham and eggs\n",
      "              Embedding 1 Closest : eggs and ham | Farthest: embeddings with mean pooling, \n",
      "              Embedding 2 Closest : eggs and ham | Farthest: representation learning is not fun\n",
      "Sentence: representation learning is fun\n",
      "              Embedding 1 Closest : representation learning is not fun | Farthest: we like simple baselines, \n",
      "              Embedding 2 Closest : we love simple baselines | Farthest: ham and eggs\n",
      "Sentence: embeddings with mean pooling\n",
      "              Embedding 1 Closest : we love simple baselines | Farthest: representation learning is not fun, \n",
      "              Embedding 2 Closest : representation learning is fun | Farthest: embeddings with mean pooling\n",
      "Sentence: we love simple baselines\n",
      "              Embedding 1 Closest : we like simple baselines | Farthest: hello world, \n",
      "              Embedding 2 Closest : we like simple baselines | Farthest: we love simple baselines\n",
      "Sentence: eggs and ham\n",
      "              Embedding 1 Closest : ham and eggs | Farthest: embeddings with mean pooling, \n",
      "              Embedding 2 Closest : ham and eggs | Farthest: representation learning is not fun\n",
      "Sentence: representation learning is not fun\n",
      "              Embedding 1 Closest : representation learning is fun | Farthest: embeddings with mean pooling, \n",
      "              Embedding 2 Closest : representation learning is fun | Farthest: eggs and ham\n",
      "Sentence: we like simple baselines\n",
      "              Embedding 1 Closest : we love simple baselines | Farthest: hello world, \n",
      "              Embedding 2 Closest : we love simple baselines | Farthest: we like simple baselines\n"
     ]
    }
   ],
   "source": [
    "compare_two_embeddings(TOY_SENTENCES+NEW_TOY_SENTENCES, sent_vecs, st_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27306227",
   "metadata": {},
   "source": [
    "Observation: <br>\n",
    "Lexical overlap dominates. <br>\n",
    "Mean pooling dilutes “not”. <br>\n",
    "SBERT variants are optimized for semantic relatedness/paraphrase rather than logical polarity. <br>\n",
    "Negation forms (“not”, “n’t”) are frequent but often get weak, context-agnostic vectors; models underrepresent negation scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d80c633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMeanPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    IDF-style Weighted-mean-pool token embeddings over non-pad tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    [B, T, D] embeddings\n",
    "        mask: [B, T]    True for real tokens\n",
    "        weight: [B, T]  IDF weight for each token\n",
    "        returns:\n",
    "            sent_emb: [B, D]\n",
    "        \"\"\"\n",
    "        mask_f = mask.float().unsqueeze(-1)     # [B, T, 1]\n",
    "        weight = weight.float().unsqueeze(-1)\n",
    "        summed = (x * mask_f * weight).sum(dim=1)        # [B, D]\n",
    "        counts = (mask_f * weight).sum(dim=1).clamp(min=1) # [B, 1]\n",
    "        return summed / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5be1d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_SENTENCES2 = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"the cat chased the mouse\",\n",
    "    \"the dog chased the ball\",\n",
    "    \"representation learning is fun\",\n",
    "    \"representation learning is not fun\",\n",
    "    \"deep learning with embeddings\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a5e3af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = build_vocab(TOY_SENTENCES2)\n",
    "ID2TOK = {i: t for t, i in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3c715a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4, 12, 10,  2,  9],\n",
       "        [ 2,  6, 12, 10,  2,  9],\n",
       "        [ 2,  4,  5,  2, 16,  0],\n",
       "        [ 2,  6,  5,  2, 13,  0],\n",
       "        [11,  3,  8,  7,  0,  0],\n",
       "        [11,  3,  8, 17,  7,  0],\n",
       "        [14,  3, 18, 15,  0,  0]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_tensor, mask = get_padded_tensor(TOY_SENTENCES2)\n",
    "ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c2f9fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18]),\n",
       " tensor([7, 8, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, counts = torch.unique(ids_tensor, return_counts=True)\n",
    "output, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13633de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1429, 0.0000, 0.1250, 0.3333, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.5000, 0.5000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = torch.max(ids_tensor).item()\n",
    "idf_vec = torch.zeros(max_id + 1)\n",
    "for k, v in zip(output, counts):\n",
    "    idf_vec[k] = 1/v\n",
    "idf_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b973316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.5000, 0.5000, 0.5000, 0.1250, 0.5000],\n",
       "        [0.1250, 0.5000, 0.5000, 0.5000, 0.1250, 0.5000],\n",
       "        [0.1250, 0.5000, 0.5000, 0.1250, 1.0000, 0.1429],\n",
       "        [0.1250, 0.5000, 0.5000, 0.1250, 1.0000, 0.1429],\n",
       "        [0.5000, 0.3333, 0.5000, 0.5000, 0.1429, 0.1429],\n",
       "        [0.5000, 0.3333, 0.5000, 1.0000, 0.5000, 0.1429],\n",
       "        [1.0000, 0.3333, 1.0000, 1.0000, 0.1429, 0.1429]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = idf_vec[ids_tensor]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecc50f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyWEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal embedding -> weighted mean pooling encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: EmbeddingModelConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim, padding_idx=cfg.pad_id)\n",
    "        self.pool = WeightedMeanPooler()\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # token_ids: [B, T]\n",
    "        output, counts = torch.unique(token_ids, return_counts=True)\n",
    "        max_id = torch.max(token_ids).item()\n",
    "        idf_vec = torch.zeros(max_id + 1)\n",
    "        for k, v in zip(output, counts):\n",
    "            idf_vec[k] = 1/v\n",
    "        weights = idf_vec[token_ids]\n",
    "        emb = self.embedding(token_ids)         # [B, T, D]\n",
    "        sent_vec = self.pool(emb, mask, weights)         # [B, D]\n",
    "        return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28386797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_weighted_mean_pooling(sentences):\n",
    "    \"\"\"\n",
    "    - Implement simple IDF-style weights (fake IDF from this tiny corpus is fine).\n",
    "    - Use weights in pooling: sum(e_i * w_i) / sum(w_i).\n",
    "    - Compare similarities vs. plain mean pooling.\n",
    "    \"\"\"\n",
    "    ids_tensor, mask = get_padded_tensor(sentences)\n",
    "\n",
    "    # Model\n",
    "    cfg = EmbeddingModelConfig(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=VOCAB[PAD_TOKEN])\n",
    "    model = TinyEmbeddingModel(cfg)\n",
    "    model_w = TinyWEmbeddingModel(cfg)\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        sent_vecs = model(ids_tensor, mask)   # [B, D]\n",
    "        sent_vecs_w = model_w(ids_tensor, mask)\n",
    "\n",
    "    compare_two_embeddings(sentences, sent_vecs, sent_vecs_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f698c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: the cat sat on the mat\n",
      "              Embedding 1 Closest : the dog sat on the mat | Farthest: representation learning is not fun, \n",
      "              Embedding 2 Closest : the dog sat on the mat | Farthest: deep learning with embeddings\n",
      "Sentence: the dog sat on the mat\n",
      "              Embedding 1 Closest : the cat sat on the mat | Farthest: representation learning is not fun, \n",
      "              Embedding 2 Closest : the cat sat on the mat | Farthest: deep learning with embeddings\n",
      "Sentence: the cat chased the mouse\n",
      "              Embedding 1 Closest : the cat sat on the mat | Farthest: representation learning is not fun, \n",
      "              Embedding 2 Closest : the dog chased the ball | Farthest: the cat chased the mouse\n",
      "Sentence: the dog chased the ball\n",
      "              Embedding 1 Closest : the cat chased the mouse | Farthest: representation learning is not fun, \n",
      "              Embedding 2 Closest : the cat chased the mouse | Farthest: the dog chased the ball\n",
      "Sentence: representation learning is fun\n",
      "              Embedding 1 Closest : representation learning is not fun | Farthest: the cat chased the mouse, \n",
      "              Embedding 2 Closest : representation learning is not fun | Farthest: the dog sat on the mat\n",
      "Sentence: representation learning is not fun\n",
      "              Embedding 1 Closest : representation learning is fun | Farthest: the cat chased the mouse, \n",
      "              Embedding 2 Closest : representation learning is fun | Farthest: the dog sat on the mat\n",
      "Sentence: deep learning with embeddings\n",
      "              Embedding 1 Closest : representation learning is not fun | Farthest: the cat chased the mouse, \n",
      "              Embedding 2 Closest : representation learning is fun | Farthest: the dog sat on the mat\n"
     ]
    }
   ],
   "source": [
    "compare_weighted_mean_pooling(TOY_SENTENCES2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f474a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class 0 = greetings\n",
    "greeting_sentences = [\n",
    "    \"hello there\",\n",
    "    \"good morning\",\n",
    "    \"hi friend\",\n",
    "    \"good evening\",\n",
    "    \"hey buddy\",\n",
    "    \"how are you\"\n",
    "]\n",
    "\n",
    "# Class 1 = food\n",
    "food_sentences = [\n",
    "    \"i love pizza\",\n",
    "    \"pasta is tasty\",\n",
    "    \"eating an apple\",\n",
    "    \"the sandwich is good\",\n",
    "    \"fresh salad\",\n",
    "    \"i like sushi\"\n",
    "]\n",
    "\n",
    "TOY_SUPERVISED = greeting_sentences + food_sentences\n",
    "TOY_LABELS = [0]*len(greeting_sentences) + [1]*len(food_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5907a84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<unk>',\n",
       " 2: 'good',\n",
       " 3: 'i',\n",
       " 4: 'is',\n",
       " 5: 'an',\n",
       " 6: 'apple',\n",
       " 7: 'are',\n",
       " 8: 'buddy',\n",
       " 9: 'eating',\n",
       " 10: 'evening',\n",
       " 11: 'fresh',\n",
       " 12: 'friend',\n",
       " 13: 'hello',\n",
       " 14: 'hey',\n",
       " 15: 'hi',\n",
       " 16: 'how',\n",
       " 17: 'like',\n",
       " 18: 'love',\n",
       " 19: 'morning',\n",
       " 20: 'pasta',\n",
       " 21: 'pizza',\n",
       " 22: 'salad',\n",
       " 23: 'sandwich',\n",
       " 24: 'sushi',\n",
       " 25: 'tasty',\n",
       " 26: 'the',\n",
       " 27: 'there',\n",
       " 28: 'you'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB = build_vocab(TOY_SUPERVISED)\n",
    "ID2TOK = {i: t for t, i in VOCAB.items()}\n",
    "ID2TOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0cbf85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(TOY_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1771f003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f54b6500b2449f2860ec11e507559ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b4f4d136ef41318ec3c26be0e03c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3731f5f8d51a4f26a69f6530dfef1756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c49818a3a44aedb37a2dbf58fefa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daa344db0194bc9a50b8fd3c4656e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16b87951e734a6298585c7460c73c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bb02c3c115498bae400b94924f7889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5697ebb9cf594ef3ab4a36e5d3d88fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f0e8e4ae6b4ee986b1463f1e2e54fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be539e940bc40a092b44a3d853781d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c60ada68b774ab7bf95f27c00d0559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "name = \"sentence-transformers/all-MiniLM-L6-v2\"  # example\n",
    "st1 = SentenceTransformer(name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "pad_id = tokenizer.pad_token_id\n",
    "pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ee0106f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb = st1[0].auto_model.get_input_embeddings()  # nn.Embedding\n",
    "pretrained_weights = word_emb.weight.detach().clone()  # [V, D]\n",
    "pretrained_weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f9c70561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyEmbeddingMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_id, trainable_embed=True, pretrained_weights=None):\n",
    "        super().__init__()\n",
    "        cfg = EmbeddingModelConfig(vocab_size=vocab_size, embed_dim=embed_dim, pad_id=pad_id)\n",
    "        self.embed_model = TinyEmbeddingModel(cfg)\n",
    "        if pretrained_weights is not None:\n",
    "            self.embed_model.embedding = nn.Embedding.from_pretrained(\n",
    "                embeddings=pretrained_weights,  # [V, D]\n",
    "                freeze=True,                    # freeze the matrix\n",
    "                padding_idx=pad_id              # PAD outputs zeros & gets no grads\n",
    "            )\n",
    "            embed_dim = pretrained_weights.shape[1]\n",
    "        self.embed_model.embedding.weight.requires_grad = trainable_embed\n",
    "        self.ln = nn.LayerNorm(embed_dim)  # learnable scale/shift\n",
    "        self.final = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [B,T] of token ids (padded with pad_id)\n",
    "        if mask is None:\n",
    "            # build mask on the fly if not provided\n",
    "            pad_id = self.embed_model.embedding.padding_idx\n",
    "            mask = x.ne(pad_id)  # True where token is real\n",
    "        emb = self.embed_model(x, mask)\n",
    "        emb = self.ln(emb)\n",
    "        y = self.final(emb).squeeze(-1)       # [B]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e9c083ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_only(X, mask, y, epochs, model, lr=1e-3, weight_decay=0.0, patience=5, min_delta=1e-3):\n",
    "    \"\"\"\n",
    "    Full-batch training for the tiny dataset.\n",
    "    X:   [B,T] token ids (padded)\n",
    "    mask:[B,T] bool mask (True for real tokens)\n",
    "    y:   [B]   labels in {0,1}\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr, weight_decay=weight_decay)\n",
    "    history = {\"train_loss\": [], \"train_acc\": []}\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    X = X.to(device)\n",
    "    mask = mask.to(device)\n",
    "    y = y.to(device).float()\n",
    "    wait = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, mask)\n",
    "        loss = criterion(logits, y) # labels: [B] in {0,1}\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            acc = (preds==y).float().mean().item()\n",
    "            avg_loss = loss.item()\n",
    "        \n",
    "        # early stopping\n",
    "        if best_loss == float(\"inf\"):\n",
    "            best_loss = avg_loss\n",
    "        else:\n",
    "            if abs(avg_loss-best_loss)<min_delta:\n",
    "                wait += 1\n",
    "            else:\n",
    "                wait = 0\n",
    "            best_loss = min(best_loss, avg_loss)\n",
    "\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stop at epoch {e+1} | best_loss={best_loss:.4f}\")\n",
    "                wait = 0\n",
    "                break\n",
    "        if wait>0 or e % 5 ==0 or e==epochs-1:\n",
    "            print(f\"Epoch {e}: train loss = {avg_loss:>6f}, train acc = {acc:>4f}\")\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"train_acc\"].append(acc)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ddc2b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = VOCAB[PAD_TOKEN]\n",
    "ids_tensor, mask = get_padded_tensor(TOY_SUPERVISED)\n",
    "labels = torch.tensor(TOY_LABELS, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81f61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13, 27,  0,  0],\n",
       "        [ 2, 19,  0,  0],\n",
       "        [15, 12,  0,  0],\n",
       "        [ 2, 10,  0,  0],\n",
       "        [14,  8,  0,  0],\n",
       "        [16,  7, 28,  0],\n",
       "        [ 3, 18, 21,  0],\n",
       "        [20,  4, 25,  0],\n",
       "        [ 9,  5,  6,  0],\n",
       "        [26, 23,  4,  2],\n",
       "        [11, 22,  0,  0],\n",
       "        [ 3, 17, 24,  0]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "041fc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.693727, train acc = 0.500000\n",
      "Epoch 5: train loss = 0.682599, train acc = 0.416667\n",
      "Epoch 10: train loss = 0.674563, train acc = 0.500000\n",
      "Epoch 15: train loss = 0.668308, train acc = 0.833333\n",
      "Epoch 20: train loss = 0.662627, train acc = 0.833333\n",
      "Epoch 25: train loss = 0.656915, train acc = 0.833333\n",
      "Epoch 30: train loss = 0.651115, train acc = 0.833333\n",
      "Epoch 35: train loss = 0.645354, train acc = 0.833333\n",
      "Epoch 40: train loss = 0.639691, train acc = 0.833333\n",
      "Epoch 45: train loss = 0.634095, train acc = 0.833333\n",
      "Epoch 49: train loss = 0.629640, train acc = 0.833333\n"
     ]
    }
   ],
   "source": [
    "model_f = TinyEmbeddingMLP(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=pad_id, trainable_embed=False, pretrained_weights=pretrained_weights)\n",
    "model_ff, _ = train_only(ids_tensor, mask, labels, 50, model_f, lr=1e-4, weight_decay=0.01, patience=5, min_delta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8e5d9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ff.to(torch.device(\"cpu\"))\n",
    "ids_tensorX = ids_tensor.to(torch.device(\"cpu\"))\n",
    "mask = mask.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d15662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent mean abs: 0.01762998104095459\n",
      "sent var mean: 3.860611468553543e-05\n",
      "unique rows: 12 / 12\n",
      "OOV fraction: 0.0\n",
      "contains PAD? True\n",
      "mask true frac: 0.6458333134651184\n",
      "final.weight requires_grad= True\n",
      "final.bias requires_grad= True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 1) Are sentence vectors non-constant?\n",
    "    sent = model_ff.embed_model(ids_tensor, mask)    # [B, D]\n",
    "    print(\"sent mean abs:\", sent.abs().mean().item())\n",
    "    print(\"sent var mean:\", sent.var(dim=0).mean().item())\n",
    "    print(\"unique rows:\", torch.unique(sent, dim=0).size(0), \"/\", sent.size(0))\n",
    "\n",
    "    # 2) OOV rate (if you have <unk>)\n",
    "    unk_id = VOCAB.get(\"<unk>\")\n",
    "    if unk_id is not None:\n",
    "        oov_frac = (ids_tensor == unk_id).float().mean().item()\n",
    "        print(\"OOV fraction:\", round(oov_frac, 3))\n",
    "\n",
    "    # 3) PAD sanity\n",
    "    pad_id = VOCAB.get(\"<pad>\")\n",
    "    print(\"contains PAD?\", bool((ids_tensor == pad_id).any()))\n",
    "    # verify mask logic: True must mean \"real token\"\n",
    "    print(\"mask true frac:\", mask.float().mean().item())\n",
    "\n",
    "    # 4) Head has grads?\n",
    "    for n, p in model_ff.named_parameters():\n",
    "        if \"final\" in n:\n",
    "            print(n, \"requires_grad=\", p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0eadbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_vs_train(sentences, labels, pretrained_weights, epochs=50, lr=1e-3, weight_decay=0.0, patience=5, min_delta=1e-3):\n",
    "    \"\"\"\n",
    "    - Create a tiny supervised objective (e.g., classify {greeting vs. food}).\n",
    "    - Compare:\n",
    "        (A) Frozen embeddings + linear head\n",
    "        (B) Trainable embeddings end-to-end\n",
    "    - Observe which converges faster / achieves higher training accuracy.\n",
    "    \"\"\"\n",
    "    pad_id = VOCAB[PAD_TOKEN]\n",
    "    ids_tensor, mask = get_padded_tensor(sentences)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # freeze\n",
    "    print(\"Train Frozen embeddings + linear head\")\n",
    "    model_f = TinyEmbeddingMLP(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=pad_id, trainable_embed=False, pretrained_weights=pretrained_weights)\n",
    "    model_f, _ = train_only(ids_tensor, mask, labels, epochs, model_f, lr=lr, weight_decay=weight_decay, patience=patience, min_delta=min_delta)\n",
    "\n",
    "    # trainable\n",
    "    print(\"Trainable embeddings end-to-end\")\n",
    "    model_t = TinyEmbeddingMLP(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=pad_id, trainable_embed=True)\n",
    "    model_t, _ = train_only(ids_tensor, mask, labels, epochs, model_t, lr=lr, weight_decay=weight_decay, patience=patience, min_delta=min_delta)\n",
    "\n",
    "    return model_f, model_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "15bb0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Frozen embeddings + linear head\n",
      "Epoch 0: train loss = 0.721469, train acc = 0.416667\n",
      "Epoch 5: train loss = 0.660002, train acc = 0.750000\n",
      "Epoch 10: train loss = 0.603310, train acc = 0.916667\n",
      "Epoch 15: train loss = 0.550458, train acc = 0.916667\n",
      "Epoch 20: train loss = 0.501928, train acc = 1.000000\n",
      "Epoch 25: train loss = 0.457929, train acc = 1.000000\n",
      "Epoch 29: train loss = 0.425927, train acc = 1.000000\n",
      "Trainable embeddings end-to-end\n",
      "Epoch 0: train loss = 0.620102, train acc = 0.500000\n",
      "Epoch 5: train loss = 0.591766, train acc = 0.833333\n",
      "Epoch 10: train loss = 0.564369, train acc = 0.833333\n",
      "Epoch 15: train loss = 0.537933, train acc = 0.833333\n",
      "Epoch 20: train loss = 0.512431, train acc = 0.916667\n",
      "Epoch 25: train loss = 0.487797, train acc = 0.916667\n",
      "Epoch 29: train loss = 0.468660, train acc = 0.916667\n"
     ]
    }
   ],
   "source": [
    "trained_f, trained_t = freeze_vs_train(TOY_SUPERVISED, TOY_LABELS, pretrained_weights, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5876d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyEmbeddingMLP(\n",
       "  (embed_model): TinyEmbeddingModel(\n",
       "    (embedding): Embedding(30522, 384, padding_idx=0)\n",
       "    (pool): MeanPooler()\n",
       "  )\n",
       "  (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (final): Linear(in_features=384, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_t.to(torch.device(\"cpu\"))\n",
    "trained_f.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8865b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: acc=0.917 | brier=0.1405 | avg_margin=0.135\n",
      "Frozen Embed: acc=1.000 | brier=0.1181 | avg_margin=0.160\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = trained_t(ids_tensor, mask)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    acc = ((probs >= 0.5)==labels).float().mean().item()\n",
    "    brier = (probs - labels.float()).pow(2).mean().item()   # calibration/CE proxy\n",
    "    margin = (probs - 0.5).abs().mean().item()         # confidence\n",
    "\n",
    "    logits2 = trained_f(ids_tensor, mask)\n",
    "    probs2 = torch.sigmoid(logits2)\n",
    "    acc2 = ((probs2 >= 0.5)==labels).float().mean().item()\n",
    "    brier2 = (probs2 - labels.float()).pow(2).mean().item()   # calibration/CE proxy\n",
    "    margin2 = (probs2 - 0.5).abs().mean().item()         # confidence\n",
    "print(f\"Trainable: acc={acc:.3f} | brier={brier:.4f} | avg_margin={margin:.3f}\")\n",
    "print(f\"Frozen Embed: acc={acc2:.3f} | brier={brier2:.4f} | avg_margin={margin2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9c8e9204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2518, 0.4355, 0.2279, 0.3937, 0.4797, 0.3974, 0.6241, 0.7083, 0.6723,\n",
       "        0.6700, 0.6255, 0.4947])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bc65b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3298, 0.2663, 0.3687, 0.3251, 0.3341, 0.4291, 0.6884, 0.6979, 0.6976,\n",
       "        0.5441, 0.6839, 0.6646])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11252d3",
   "metadata": {},
   "source": [
    "### Try 2-stage warm start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b369d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_warm_start(X, mask, y, model, epochs):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    history = {\"train_loss\": [], \"train_acc\": []}\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    X = X.to(device)\n",
    "    mask = mask.to(device)\n",
    "    y = y.to(device).float()\n",
    "    wait = 0\n",
    "\n",
    "\n",
    "    # Stage 1: freeze embeddings, train head only\n",
    "    for p in model.embed_model.embedding.parameters(): p.requires_grad = False\n",
    "    optimizer = torch.optim.Adam(model.final.parameters(), lr=5e-3)\n",
    "\n",
    "    for e in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, mask)\n",
    "        loss = criterion(logits, y) # labels: [B] in {0,1}\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            acc = (preds==y).float().mean().item()\n",
    "            avg_loss = loss.item()\n",
    "        \n",
    "        print(f\"Epoch {e}: train loss = {avg_loss:>6f}, train acc = {acc:>4f}\")\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"train_acc\"].append(acc)\n",
    "\n",
    "    # Stage 2: unfreeze with smaller LR for emb, larger for head\n",
    "    for p in model.embed_model.embedding.parameters(): p.requires_grad = True\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": model.embed_model.embedding.parameters(), \"lr\": 5e-4},\n",
    "        {\"params\": list(model.final.parameters()) + list(model.ln.parameters()), \"lr\": 5e-3},\n",
    "    ], weight_decay=1e-4)\n",
    "\n",
    "    for e in range(10, epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, mask)\n",
    "        loss = criterion(logits, y) # labels: [B] in {0,1}\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            acc = (preds==y).float().mean().item()\n",
    "            avg_loss = loss.item()\n",
    "        \n",
    "        # early stopping\n",
    "        if best_loss == float(\"inf\"):\n",
    "            best_loss = avg_loss\n",
    "        else:\n",
    "            if abs(avg_loss-best_loss)<1e-3:\n",
    "                wait += 1\n",
    "            else:\n",
    "                wait = 0\n",
    "            best_loss = min(best_loss, avg_loss)\n",
    "\n",
    "            if wait >= 5:\n",
    "                print(f\"Early stop at epoch {e+1} | best_loss={best_loss:.4f}\")\n",
    "                wait = 0\n",
    "                break\n",
    "        if wait>0 or e % 5 ==0 or e==epochs-1:\n",
    "            print(f\"Epoch {e}: train loss = {avg_loss:>6f}, train acc = {acc:>4f}\")\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"train_acc\"].append(acc)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d5d8c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.736225, train acc = 0.333333\n",
      "Epoch 1: train loss = 0.713766, train acc = 0.416667\n",
      "Epoch 2: train loss = 0.691947, train acc = 0.416667\n",
      "Epoch 3: train loss = 0.670756, train acc = 0.666667\n",
      "Epoch 4: train loss = 0.650195, train acc = 0.666667\n",
      "Epoch 5: train loss = 0.630271, train acc = 0.666667\n",
      "Epoch 6: train loss = 0.610988, train acc = 0.666667\n",
      "Epoch 7: train loss = 0.592346, train acc = 0.666667\n",
      "Epoch 8: train loss = 0.574341, train acc = 0.666667\n",
      "Epoch 9: train loss = 0.556966, train acc = 0.666667\n",
      "Epoch 10: train loss = 0.540208, train acc = 0.750000\n",
      "Epoch 15: train loss = 0.451410, train acc = 0.916667\n",
      "Epoch 20: train loss = 0.376267, train acc = 0.916667\n",
      "Epoch 25: train loss = 0.312388, train acc = 1.000000\n",
      "Epoch 30: train loss = 0.258114, train acc = 1.000000\n",
      "Epoch 35: train loss = 0.212386, train acc = 1.000000\n",
      "Epoch 40: train loss = 0.174256, train acc = 1.000000\n",
      "Epoch 45: train loss = 0.142873, train acc = 1.000000\n",
      "Epoch 49: train loss = 0.121967, train acc = 1.000000\n"
     ]
    }
   ],
   "source": [
    "model_t_warm = TinyEmbeddingMLP(vocab_size=len(VOCAB), embed_dim=EMBED_DIM, pad_id=pad_id, trainable_embed=True)\n",
    "model_t_warm = train_warm_start(ids_tensor, mask, labels, model_t_warm, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "57837581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: acc=1.000 | brier=0.0142 | avg_margin=0.391\n"
     ]
    }
   ],
   "source": [
    "model_t_warm.to(torch.device(\"cpu\"))\n",
    "with torch.no_grad():\n",
    "    logits = model_t_warm(ids_tensor, mask)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    acc = ((probs >= 0.5)==labels).float().mean().item()\n",
    "    brier = (probs - labels.float()).pow(2).mean().item()   # calibration/CE proxy\n",
    "    margin = (probs - 0.5).abs().mean().item()         # confidence\n",
    "\n",
    "print(f\"Trainable: acc={acc:.3f} | brier={brier:.4f} | avg_margin={margin:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
