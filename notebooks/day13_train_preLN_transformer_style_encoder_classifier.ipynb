{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425813ff",
   "metadata": {},
   "source": [
    "# Stability & Optimization lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c6bd8",
   "metadata": {},
   "source": [
    "Making training smooth and reliable:\n",
    "\n",
    "- Norm choices: LayerNorm vs RMSNorm\n",
    "\n",
    "- Residual wiring: plain vs scaled (α) vs ReZero gating\n",
    "\n",
    "- Regularization: weight decay (AdamW), label smoothing\n",
    "\n",
    "- Training knobs: LRs, (optional) warmup, grad clipping\n",
    "\n",
    "- Masking sanity (padding/causal) to avoid silent bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bdfd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b17d36",
   "metadata": {},
   "source": [
    "## Drop-in RMSNorm + gated residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d239e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Norms -----\n",
    "class RMSNorm(nn.Module):\n",
    "    # LN: learnable scale + bias (per feature), with mean-centering and variance normalization.\n",
    "    # RMSNorm: learnable scale only (per feature), with RMS normalization (no centering)\n",
    "    def __init__(self, d, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # normalize by root-mean-square over features\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
    "        x_hat = x / rms\n",
    "        return self.scale * x_hat\n",
    "\n",
    "def make_norm(kind: str, d_model: int):\n",
    "    kind = kind.lower()\n",
    "    if kind in (\"ln\", \"layernorm\"):\n",
    "        return nn.LayerNorm(d_model)\n",
    "    elif kind in (\"rms\", \"rmsnorm\"):\n",
    "        return RMSNorm(d_model)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown norm: {kind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a736b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Residual wrappers -----\n",
    "class ResidualAdd(nn.Module):\n",
    "    \"\"\"x + F(LN(x)) with optional scaling or gating (residual-zero).\"\"\"\n",
    "    def __init__(self, d_model, fn, norm=\"ln\", drop_p=0.1, mode=\"plain\"):\n",
    "        super().__init__()\n",
    "        self.norm = make_norm(norm, d_model)\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.mode = mode\n",
    "        if mode == \"scaled\":         # constant residual scale α\n",
    "            self.alpha = 0.5\n",
    "        elif mode == \"rezero\":       # learnable gate g, init 0\n",
    "            # let training “turn on” the residuals, stabilizes very deep stacks and can reduce the need for aggressive warmup or heavy normalization\n",
    "            self.g = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x, **kwargs):\n",
    "        h = self.fn(self.norm(x), **kwargs)\n",
    "        h = self.drop(h)\n",
    "        if self.mode == \"plain\":\n",
    "            return x + h\n",
    "        elif self.mode == \"scaled\":\n",
    "            return x + self.alpha * h\n",
    "        elif self.mode == \"rezero\":\n",
    "            return x + self.g * h\n",
    "        else:\n",
    "            raise ValueError(self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43bcb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor my preLN encoder\n",
    "class PreLNEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p_drop=0.1, ff_multi=4,\n",
    "                 norm=\"ln\", resid_mode=\"plain\"):\n",
    "        super().__init__()\n",
    "        # use nn.MultiheadAttention wrapper\n",
    "        self.mha_ctor = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=p_drop, batch_first=True)\n",
    "\n",
    "        self.attn = ResidualAdd(\n",
    "            d_model,\n",
    "            # # out, attn = mha(x, pad_mask=..., causal=...), we only need the out, hence [0].\n",
    "            fn=lambda x, pad_mask=None, causal=False, attn_mask=None: \n",
    "            self.mha_ctor(x, x, x, \n",
    "                     key_padding_mask=(~pad_mask) if pad_mask is not None else None, \n",
    "                     is_causal=causal, \n",
    "                     attn_mask=attn_mask, \n",
    "                     need_weights=False)[0], \n",
    "            norm=norm, drop_p=p_drop, mode=resid_mode\n",
    "        )\n",
    "        self.ff = ResidualAdd(\n",
    "            d_model,\n",
    "            fn=nn.Sequential(\n",
    "                nn.Linear(d_model, ff_multi*d_model),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(ff_multi*d_model, d_model),\n",
    "            ),\n",
    "            norm=norm, drop_p=p_drop, mode=resid_mode\n",
    "        )\n",
    "    def forward(self, x, pad_mask=None, causal=False, attn_mask=None):\n",
    "        assert pad_mask.dtype == torch.bool\n",
    "        x = self.attn(x, pad_mask=pad_mask, causal=causal, attn_mask=attn_mask) # [B,T,D]\n",
    "        x = self.ff(x) # [B,T,D]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c346e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE helpers\n",
    "class SinusoidalPE(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 256):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)               # [Tmax, D]\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # [Tmax, 1]\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [D/2]\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe)                   # not trainable\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # x: [B,T,D]\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:T].unsqueeze(0) \n",
    "    \n",
    "class ClippedRelPosBias(nn.Module):\n",
    "    def __init__(self, num_heads, max_rel=128):\n",
    "        super().__init__()\n",
    "        self.max_rel = max_rel\n",
    "        self.table = nn.Parameter(torch.zeros(num_heads, 2*max_rel - 1))  # [-R+1..R-1]\n",
    "\n",
    "    def forward(self, T, *, device=None):\n",
    "        device = device or self.table.device\n",
    "        q = torch.arange(T, device=device)[:, None]\n",
    "        k = torch.arange(T, device=device)[None, :]\n",
    "        rel = (k - q).clamp(-self.max_rel+1, self.max_rel-1)  # [T,T]\n",
    "        idx = rel + (self.max_rel - 1)          # shift to [0..2R-2]\n",
    "        bias = self.table[:, idx]               # [H,T,T]\n",
    "        return bias\n",
    "        # nn.MultiheadAttention's attn_mask accepts [B*H, T, T] (per (batch, head) bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d42955f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder→Pool→Head wrapper\n",
    "class MHAEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, pad_id, cls_id, num_heads, pe= \"none\", # {\"none\",\"sin\",\"relbias\"}\n",
    "                 p_drop=0.1, ff_multi=4, norm=\"ln\", resid_mode=\"plain\", num_layers=2, pool=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.cls_id = cls_id   # int or None\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pe = pe\n",
    "        if pe==\"sin\":\n",
    "            self.posenc = SinusoidalPE(d_model) # returns x + pe(x)\n",
    "        elif pe==\"relbias\":\n",
    "            self.posenc=ClippedRelPosBias(num_heads) # returns [H,T,T]\n",
    "        self.block = nn.ModuleList([PreLNEncoderBlock(d_model, num_heads, p_drop, ff_multi, norm, resid_mode) for _ in range(num_layers)])\n",
    "        self.final_ln = make_norm(norm, d_model)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "        self.pool = pool\n",
    "\n",
    "    def masked_mean(self, x, mask):               # x:[B,T,D], mask:[B,T]\n",
    "        m = mask.float().unsqueeze(-1)\n",
    "        return (x*m).sum(1) / m.sum(1).clamp(min=1.0)\n",
    "\n",
    "    def forward(self, ids: torch.Tensor, mask: torch.Tensor):\n",
    "        attn_mask = None # default\n",
    "        if self.pool == \"cls\" and self.cls_id is not None:\n",
    "            # prepend CLS id and mask=True (non-pad)\n",
    "            cls_col = torch.full((B, 1), self.cls_id, dtype=ids.dtype, device=ids.device)\n",
    "            ids = torch.cat([cls_col, ids], dim=1)                             # [B,T+1]\n",
    "            mask = torch.cat([torch.ones(B,1, dtype=torch.bool, device=mask.device), mask], dim=1)  # [B,T+1]\n",
    "\n",
    "        x = self.embed(ids)                       # [B,T,D]\n",
    "        B,T,_ = x.shape\n",
    "        # pe\n",
    "        if self.pe==\"sin\":\n",
    "            x = self.posenc(x)\n",
    "        elif self.pe==\"relbias\":\n",
    "            bias = self.posenc(T, device=x.device)\n",
    "            # [H, T, T] -> [1, H, T, T] -> [B, H, T, T] -> [B*H, T, T]\n",
    "            attn_mask = bias.unsqueeze(0).expand(B, -1, -1, -1).reshape(B*self.num_heads, T, T)\n",
    "        for b in self.block:\n",
    "            x = b(x, pad_mask=mask, attn_mask=attn_mask, causal=False)             # [B,T,D]\n",
    "        x = self.final_ln(x)\n",
    "\n",
    "        if self.pool == \"mean\":\n",
    "            sent = self.masked_mean(x, mask)      # [B,D]\n",
    "        else:\n",
    "            sent = x[:, 0, :]                     # first-token (CLS-style)\n",
    "        logits = self.head(sent).view(-1)      # [B]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Binary label smoothing helper (for BCE)\n",
    "# Use it once per batch before computing BCEWithLogitsLoss.\n",
    "def smooth_labels(y, eps=0.05):\n",
    "    # y in {0,1} -> {eps, 1-eps}\n",
    "    return y*(1-eps) + (1-y)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42dced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & (optional) warmup\n",
    "# Param groups that exclude biases/norms from weight decay\n",
    "def adamw_groups(model, lr_enc, lr_head, wd_enc=1e-4, wd_head=0.0):\n",
    "    enc_decay, enc_no_decay, head_decay, head_no_decay = [], [], [], []\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        is_head = n.startswith(\"head.\")\n",
    "        is_bias = n.endswith(\".bias\")\n",
    "        is_norm = (\"norm\" in n.lower())  # catches LayerNorm/RMSNorm params\n",
    "        # treat other scale-like knobs as no_decay too:\n",
    "        is_scale_like = n.endswith(\".scale\") or n.endswith(\".g\") # scale or gating\n",
    "        is_pos_emb = (\"pos\" in n.lower() and \"emb\" in n.lower()) # positional embeddings\n",
    "\n",
    "        no_decay = is_bias or is_norm or is_scale_like or is_pos_emb\n",
    "\n",
    "        if is_head:\n",
    "            (head_no_decay if no_decay else head_decay).append(p)\n",
    "        else:\n",
    "            (enc_no_decay  if no_decay else enc_decay ).append(p)\n",
    "\n",
    "    groups = []\n",
    "    if enc_decay:\n",
    "        groups.append({\"params\": enc_decay,     \"lr\": lr_enc,  \"weight_decay\": wd_enc})\n",
    "    if enc_no_decay:\n",
    "        groups.append({\"params\": enc_no_decay,  \"lr\": lr_enc,  \"weight_decay\": 0.0})\n",
    "    if head_decay:\n",
    "        groups.append({\"params\": head_decay,    \"lr\": lr_head, \"weight_decay\": wd_head})\n",
    "    if head_no_decay:\n",
    "        groups.append({\"params\": head_no_decay, \"lr\": lr_head, \"weight_decay\": 0.0})\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49ce823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup (linear) → constant:\n",
    "def linear_warmup(step, warmup_steps, base_lr):\n",
    "    if step < warmup_steps:\n",
    "        return base_lr * (step+1)/warmup_steps\n",
    "    return base_lr\n",
    "\n",
    "# inside training loop:\n",
    "# step += 1\n",
    "# for g in opt.param_groups: g[\"lr\"] = linear_warmup(step, warmup_steps=200, base_lr=g[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1ac4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ctor that returns a *fresh* model each fold\n",
    "def make_model(vocab_size, d_model, pad_id, cls_id, num_heads, pe, p_drop, ff_multi, norm, resid_mode, num_layers, pool):\n",
    "    return MHAEncoderClassifier(vocab_size=vocab_size, d_model=d_model, pad_id=pad_id, cls_id=cls_id,\n",
    "                                num_heads=num_heads, pe=pe, p_drop=p_drop, ff_multi=ff_multi,\n",
    "                                norm=norm, resid_mode=resid_mode, num_layers=num_layers, pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ecb9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _call_model(model, Xb, Mb=None):\n",
    "    \"\"\"Call model with (X, mask) if mask given; unwrap (logits, extra) if returned.\"\"\"\n",
    "    if Mb is None:\n",
    "        out = model(Xb)\n",
    "    else:\n",
    "        out = model(Xb, Mb)\n",
    "    if isinstance(out, (tuple, list)): # out, attn\n",
    "        logits = out[0]\n",
    "    else:\n",
    "        logits = out\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78e631fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(X, y, M, model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = _call_model(model, X, M).squeeze(-1)\n",
    "        y_flt  = y.float()\n",
    "        loss = criterion(logits, y_flt)\n",
    "        prob = torch.sigmoid(loss)\n",
    "        preds = (prob >= 0.5).float()\n",
    "        # acc\n",
    "        acc = (preds==y).float().mean().item()\n",
    "        # Brier (binary): mean (p - y)^2\n",
    "        brier = torch.mean(torch.pow(preds-y_flt,2)).item()\n",
    "        # avg margin around 0.5 (simple): mean |p - 0.5|\n",
    "        margin = torch.mean(torch.abs(preds-0.5)).item()\n",
    "\n",
    "    return {\"loss\": loss.item(), \"acc\": acc, \"brier\": brier, \"margin\": margin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eac37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "07ff1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(X, y, vocab_size, pad_id, mask=None, cls_id = None, *,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"ln\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=0,\n",
    "    seed=42,):\n",
    "\n",
    "    assert X.shape[0] == y.shape[0], \"X and y must align\"\n",
    "    if mask is not None:\n",
    "        assert mask.shape[0] == y.shape[0], \"mask and y must align\"\n",
    "    device = device or (torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "                        else torch.device(\"cpu\"))\n",
    "    \n",
    "    # move once to device\n",
    "    X = X.to(device)\n",
    "    y_long = y.to(device).long()   # keep an integer copy for stratify + metrics\n",
    "    y_flt  = y_long.float()        # float copy for BCE\n",
    "    M = mask.to(device) if mask is not None else None\n",
    "\n",
    "    # Global seeding (once)\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    all_metrics = []\n",
    "\n",
    "    # scikit-learn needs a NumPy array on CPU, the 1 makes fold numbers start at 1 instead of 0.\n",
    "    y_np = y_long.detach().cpu().numpy()\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(y_np)), y_np), 1):\n",
    "        tr_idx = torch.tensor(tr_idx, device=device)\n",
    "        va_idx = torch.tensor(va_idx, device=device)\n",
    "        # model, criterion, optimizer\n",
    "        model = model_ctor(vocab_size, d_model, pad_id, cls_id, num_heads, pe, p_drop, ff_multi, norm, resid_mode, num_layers, pool).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        param_groups = groups_fn(model, lr_enc=lr_enc, lr_head=lr_head or lr_enc, wd_enc=wd_enc, wd_head=wd_head)\n",
    "        Optim = torch.optim.AdamW if use_adamw else torch.optim.Adam\n",
    "        opt = Optim(param_groups, betas=(0.9,0.98), eps=1e-9)\n",
    "\n",
    "        for g in opt.param_groups:\n",
    "            g.setdefault(\"base_lr\", g[\"lr\"])\n",
    "        global_step = 0\n",
    "        \n",
    "        best = math.inf\n",
    "        best_sd = None\n",
    "        wait = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            # build batch\n",
    "            if batch_size is None or batch_size >= len(tr_idx):\n",
    "                batches = [tr_idx]\n",
    "            else:\n",
    "                perm = tr_idx[torch.randperm(len(tr_idx), device=device)]\n",
    "                batches = [perm[i:i+batch_size] for i in range(0, len(perm), batch_size)]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            for bi in batches:\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                # compute logit\n",
    "                logit = _call_model(model,X[bi], M[bi] if M is not None else None)\n",
    "                targets = y_flt[bi]\n",
    "                # smooth_labels(y)\n",
    "                if smooth_eps>0:\n",
    "                    targets = smooth_labels(targets, eps=smooth_eps)\n",
    "                # compute loss\n",
    "                loss = criterion(logit, targets)\n",
    "                loss.backward()\n",
    "                total_loss += loss.item() * len(bi)\n",
    "                # prevents exploding gradients by rescaling all gradients so their total norm ≤ clip\n",
    "                if clip:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                global_step += 1\n",
    "                for g in opt.param_groups:\n",
    "                    g[\"lr\"] = linear_warmup(global_step, warmup_steps=warmup_steps, base_lr=g[\"base_lr\"])\n",
    "                opt.step()\n",
    "            train_loss = total_loss / len(tr_idx)\n",
    "\n",
    "            # evaluate on val\n",
    "            val_dict = _eval(X[va_idx], y_long[va_idx], M[va_idx] if M is not None else None, model, criterion)\n",
    "            if epoch % 5 ==0:\n",
    "                print(f\"Fold {fold}, epoch {epoch}, {[k+': '+str(v) for k,v in val_dict.items()]}\")\n",
    "            # early stopping on val\n",
    "            if best-val_dict['loss']<min_delta:\n",
    "                wait +=1\n",
    "            else:\n",
    "                wait = 0\n",
    "            if val_dict['loss']<best:\n",
    "                best = val_dict['loss']\n",
    "                best_sd = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            if wait>=patience:\n",
    "                break\n",
    "\n",
    "        # restore best\n",
    "        if best_sd is not None:\n",
    "            model.load_state_dict(best_sd)\n",
    "        # log final metrics on this fold with best model\n",
    "        fold_metrics = _eval(X[va_idx], y_long[va_idx], M[va_idx] if M is not None else None, model, criterion)\n",
    "        all_metrics.append(fold_metrics)\n",
    "\n",
    "    # aggregate all folds\n",
    "    keys = all_metrics[0].keys()\n",
    "    mean = {k: float(np.mean([m[k] for m in all_metrics])) for k in keys}\n",
    "    std  = {k: float(np.std( [m[k] for m in all_metrics])) for k in keys}\n",
    "    return {\"folds\": all_metrics, \"mean\": mean, \"std\": std}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6736062",
   "metadata": {},
   "source": [
    "| Variant | Norm      | Residual     | Loss                   | Optim             |\n",
    "| :-----: | --------- | ------------ | ---------------------- | ----------------- |\n",
    "|    A    | LayerNorm | plain        | BCE                    | Adam              |\n",
    "|    B    | RMSNorm   | plain        | BCE                    | AdamW             |\n",
    "|    C    | RMSNorm   | scaled α=0.5 | BCE + smoothing ε=0.05 | AdamW             |\n",
    "|    D    | RMSNorm   | ReZero       | BCE                    | AdamW (no warmup) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8583ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_hard = [\n",
    "    \"good morning everyone\",\n",
    "    \"hello there my friend\",\n",
    "    \"hey buddy how are you\",\n",
    "    \"good evening folks\",\n",
    "    \"salutations from the sushi bar\",          # greeting + food word\n",
    "    \"pizza party greetings to all\",            # greeting + food word\n",
    "    \"hi from the ramen shop\",                  # greeting + food word\n",
    "    \"hello and welcome to brunch\",             # greeting + food word\n",
    "]\n",
    "food_hard = [\n",
    "    \"i love pizza\",\n",
    "    \"pasta is tasty tonight\",\n",
    "    \"fresh salad with apple\",\n",
    "    \"i like sushi a lot\",\n",
    "    \"good sandwich this morning\",              # food + greeting words\n",
    "    \"ramen is great hello world\",              # food + greeting word\n",
    "    \"eating an apple for breakfast\",\n",
    "    \"not a fan of pizza anymore\",              # negation\n",
    "]\n",
    "HARD_SUP = greeting_hard + food_hard\n",
    "HARD_LABELS = [0]*len(greeting_hard) + [1]*len(food_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8fe6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "CLS_TOKEN = \"<cls>\"\n",
    "\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    freq = {}\n",
    "    for s in sentences:\n",
    "        for w in s.split():\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    words = [w for w, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])) if c >= min_freq]\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for w in words:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def build_vocab_with_specials(sentences):\n",
    "    base = build_vocab(sentences)\n",
    "    # reserve 0/1/2 = <pad>/<unk>/<cls>\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1, CLS_TOKEN: 2}\n",
    "    next_id = 3\n",
    "    for w in base:\n",
    "        if w in (PAD_TOKEN, UNK_TOKEN):  # skip if already special\n",
    "            continue\n",
    "        vocab[w] = next_id\n",
    "        next_id += 1\n",
    "    return vocab\n",
    "\n",
    "# Build vocab with <cls>\n",
    "VOCAB = build_vocab_with_specials(HARD_SUP)\n",
    "pad_id, cls_id = VOCAB[PAD_TOKEN], VOCAB[CLS_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "064dc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s): return s.strip().split()\n",
    "\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]\n",
    "\n",
    "def pad_batch(batch_ids, pad_id=0):\n",
    "    T = max(len(x) for x in batch_ids)\n",
    "    padded, mask = [], []\n",
    "    for ids in batch_ids:\n",
    "        pad_len = T - len(ids)\n",
    "        padded.append(ids + [pad_id]*pad_len)\n",
    "        mask.append([1]*len(ids) + [0]*pad_len)\n",
    "    return torch.tensor(padded, dtype=torch.long), torch.tensor(mask, dtype=torch.bool)\n",
    "\n",
    "# dataset → tensors \n",
    "def make_tensors(sentences, labels, vocab, pad_id):\n",
    "    tok = [tokenize(s) for s in sentences]\n",
    "    ids = [numericalize(t, vocab) for t in tok]\n",
    "    X, M = pad_batch(ids, pad_id)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    return X, M, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ad9ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, M, y = make_tensors(HARD_SUP, HARD_LABELS, VOCAB, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5aaf822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5a4358a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 6]), torch.Size([16, 6]), torch.Size([16]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, M.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31b8d16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(len(VOCAB), 32, padding_idx=pad_id)\n",
    "embed(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f2fd8fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.6099370718002319', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.3418367803096771', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.1882925182580948', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.14169655740261078', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.1305415779352188', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.1261541098356247', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 30, ['loss: 0.12303049117326736', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 35, ['loss: 0.12025728076696396', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 40, ['loss: 0.11739969998598099', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6846072673797607', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.9514030814170837', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.6246516108512878', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.45326921343803406', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.5880377292633057,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.11739969998598099,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6298583149909973,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6424539685249329,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.45326921343803406,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.48620378524065017,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.19622539854449506,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"ln\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=False,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=1,\n",
    "    seed=42,)\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa3dcdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.7572334408760071', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.8552422523498535', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6297099590301514', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.5862617492675781', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 1.0515602827072144', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.6719502210617065,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.5542317032814026,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6280735731124878,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.5338841676712036,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.8048624992370605,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.6386004328727722,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.09690167861395321,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"rms\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=1,\n",
    "    warmup_steps=0,\n",
    "    seed=42,)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b7e7082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.6311007142066956', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.4173022210597992', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.28249046206474304', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.16031070053577423', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.12056028842926025', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.13040964305400848', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6759376525878906', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.7648405432701111', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.7301142811775208', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.6789421439170837', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 15, ['loss: 0.5963544249534607', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 20, ['loss: 0.49108779430389404', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 25, ['loss: 0.4295191764831543', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 30, ['loss: 0.4103851020336151', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 35, ['loss: 0.41052913665771484', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.6285253763198853,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.11987989395856857,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6319447159767151,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6227778792381287,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.4085395634174347,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.48233348578214646,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.20014912736074122,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3 = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"rms\", \n",
    "    resid_mode=\"scaled\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0.05,\n",
    "    warmup_steps=1,\n",
    "    seed=42,)\n",
    "v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6048f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.6179345846176147', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 1, epoch 10, ['loss: 0.6171682476997375', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.512432336807251', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.482908695936203', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.4404577314853668', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.3761892318725586', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.2831224203109741', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 30, ['loss: 0.1605003923177719', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 35, ['loss: 0.06715098023414612', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 40, ['loss: 0.030630091205239296', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6390504240989685', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 10, ['loss: 0.6414701342582703', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.6356987357139587', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.7567729353904724', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.753259003162384', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 15, ['loss: 0.7494342923164368', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 20, ['loss: 0.7470483183860779', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.6169049739837646,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.030630091205239296,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6387419700622559,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6206154227256775,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.7470483183860779,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.530788155272603,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.25458421701752676,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v4 = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"rms\", \n",
    "    resid_mode=\"rezero\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=0,\n",
    "    seed=42,)\n",
    "v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "861fe735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.6081080436706543', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.3438802659511566', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.1896723508834839', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.14223147928714752', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.13118593394756317', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.12694151699543', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 30, ['loss: 0.12393798679113388', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 35, ['loss: 0.12127849459648132', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 40, ['loss: 0.11863742023706436', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6819294095039368', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.9476330876350403', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.6251329183578491', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.451951265335083', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.587480366230011,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.11863742023706436,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6298519968986511,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6422449946403503,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.451951265335083,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.48603320866823196,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.19571343128173535,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1b = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=0.0,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"ln\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=1,\n",
    "    seed=42,)\n",
    "v1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9b4ed9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 0.608108401298523', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.3438807427883148', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.18967290222644806', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.14223168790340424', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.13118627667427063', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.12694230675697327', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 30, ['loss: 0.12393901497125626', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 35, ['loss: 0.12127966433763504', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 40, ['loss: 0.11863859742879868', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 0.6819294095039368', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 0.9476321339607239', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.6251330971717834', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.45195162296295166', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.5874805450439453,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.11863859742879868,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6298520565032959,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6422448754310608,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.45195162296295166,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.48603353947401046,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.19571298512595783,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1c = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=None,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"ln\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=1,\n",
    "    seed=42,)\n",
    "v1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1597b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, epoch 5, ['loss: 1.2776756286621094', 'acc: 0.5', 'brier: 0.5', 'margin: 0.5']\n",
      "Fold 2, epoch 5, ['loss: 0.04318267107009888', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 10, ['loss: 0.041952747851610184', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 15, ['loss: 0.029705777764320374', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 20, ['loss: 0.020358499139547348', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 25, ['loss: 0.014227413572371006', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 30, ['loss: 0.009127969853579998', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 35, ['loss: 0.005400742869824171', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 2, epoch 40, ['loss: 0.0035138383973389864', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 3, epoch 5, ['loss: 1.5727739334106445', 'acc: 0.3333333432674408', 'brier: 0.6666666865348816', 'margin: 0.5']\n",
      "Fold 4, epoch 5, ['loss: 1.8163561820983887', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 5, ['loss: 0.11026135087013245', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 10, ['loss: 0.02113119699060917', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n",
      "Fold 5, epoch 15, ['loss: 0.025463616475462914', 'acc: 0.6666666865348816', 'brier: 0.3333333432674408', 'margin: 0.5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folds': [{'loss': 0.6329571008682251,\n",
       "   'acc': 0.5,\n",
       "   'brier': 0.5,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.0035138383973389864,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.6062131524085999,\n",
       "   'acc': 0.3333333432674408,\n",
       "   'brier': 0.6666666865348816,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 1.0578625202178955,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5},\n",
       "  {'loss': 0.02071155607700348,\n",
       "   'acc': 0.6666666865348816,\n",
       "   'brier': 0.3333333432674408,\n",
       "   'margin': 0.5}],\n",
       " 'mean': {'loss': 0.4642516335938126,\n",
       "  'acc': 0.5000000119209289,\n",
       "  'brier': 0.5000000119209289,\n",
       "  'margin': 0.5},\n",
       " 'std': {'loss': 0.402491144875818,\n",
       "  'acc': 0.14907120294265402,\n",
       "  'brier': 0.149071202942654,\n",
       "  'margin': 0.0}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini batch\n",
    "v1d = kfold_train(X, y, vocab_size=len(VOCAB), pad_id=pad_id, cls_id=cls_id, mask=M,\n",
    "    model_ctor=make_model,                # callable: () -> fresh model\n",
    "    device=None,\n",
    "    n_splits=5,\n",
    "    epochs=40,\n",
    "    batch_size=2,           # None = full-batch\n",
    "    lr_enc=3e-3,\n",
    "    lr_head=None,              # if None, use lr_enc for all params\n",
    "    wd_enc=1e-4,\n",
    "    wd_head=0.0,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "    clip=1.0,\n",
    "    d_model=32, \n",
    "    num_heads=4, \n",
    "    pe= \"none\",\n",
    "    p_drop=0.1, \n",
    "    ff_multi=4,\n",
    "    norm=\"ln\", \n",
    "    resid_mode=\"plain\", \n",
    "    num_layers=2,\n",
    "    pool=\"mean\",\n",
    "    use_adamw=True,\n",
    "    groups_fn=adamw_groups,            # optional: callable(model, lr_enc, lr_head, wd_enc, wd_head) -> param_groups\n",
    "    smooth_eps=0,\n",
    "    warmup_steps=1,\n",
    "    seed=42,)\n",
    "v1d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
