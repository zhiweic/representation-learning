{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61635487",
   "metadata": {},
   "source": [
    "# Implement scaled dot-product attention for attention-pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec33c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c347e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "EMBED_DIM = 32\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# Tiny corpus from Day 8 + a few negations\n",
    "TOY_SENTENCES = [\n",
    "    \"hello there\",\n",
    "    \"good morning\",\n",
    "    \"hi friend\",\n",
    "    \"good evening\",\n",
    "    \"hey buddy\",\n",
    "    \"how are you\",\n",
    "    \"i love pizza\",\n",
    "    \"pasta is tasty\",\n",
    "    \"eating an apple\",\n",
    "    \"the sandwich is good\",\n",
    "    \"fresh salad\",\n",
    "    \"i like sushi\",\n",
    "    \"representation learning is fun\",\n",
    "    \"representation learning is not fun\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b9134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences: List[str], min_freq: int = 1) -> Dict[str, int]:\n",
    "    freq = {}\n",
    "    for s in sentences:\n",
    "        for w in s.split():\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    words = [w for w, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])) if c >= min_freq]\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for w in words:\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def tokenize(s: str) -> List[str]: return s.strip().split()\n",
    "\n",
    "def numericalize(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n",
    "    return [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]\n",
    "\n",
    "def pad_batch(batch_ids: List[List[int]], pad_id: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    T = max(len(x) for x in batch_ids)\n",
    "    padded, mask = [], []\n",
    "    for ids in batch_ids:\n",
    "        pad_len = T - len(ids)\n",
    "        padded.append(ids + [pad_id] * pad_len)\n",
    "        mask.append([1] * len(ids) + [0] * pad_len)\n",
    "    X = torch.tensor(padded, dtype=torch.long)\n",
    "    M = torch.tensor(mask, dtype=torch.bool)\n",
    "    return X, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd994e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = build_vocab(TOY_SENTENCES)\n",
    "ID2TOK = {i: t for t, i in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746ebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pool Encoder\n",
    "class MeanPooler(nn.Module):\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        maskf = mask.float().unsqueeze(-1)   # [B,T,1]\n",
    "        summed = (x * maskf).sum(1)          # [B,D]\n",
    "        count = maskf.sum(1).clamp(min=1.0)  # [B,1]\n",
    "        return summed / count\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingModelConfig:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    pad_id: int\n",
    "\n",
    "class TinyEmbeddingModel(nn.Module):\n",
    "    def __init__(self, cfg: EmbeddingModelConfig):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim, padding_idx=cfg.pad_id)\n",
    "        self.pool = MeanPooler()\n",
    "    def forward(self, token_ids: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.embedding(token_ids)      # [B,T,D]\n",
    "        return self.pool(emb, mask)          # [B,D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b8613",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Uses one learnable global query vector q.\n",
    "class ScaledDotAttentionPooler(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Parameter(torch.randn(dim))   # global query [D]\n",
    "        self.Wk = nn.Linear(dim, dim, bias=False) # maps token embeddings into a key space attuned to what the query will score\n",
    "        self.Wv = nn.Linear(dim, dim, bias=False) # maps token embeddings into a value space that will be mixed according to those scores.\n",
    "        # These are standard linear projections in attention; \n",
    "        # they let the model learn “what makes a token match the query” (via keys) \n",
    "        # and “what information to collect” (via values).\n",
    "        self.scale = math.sqrt(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    [B,T,D] token embeddings\n",
    "        mask: [B,T]   True for real tokens\n",
    "        returns: [B,D] attended sentence vectors\n",
    "        \"\"\"\n",
    "        K = self.Wk(x)                             # [B,T,D]\n",
    "        V = self.Wv(x)                             # [B,T,D]\n",
    "        # Broadcast query to batch: Q = [B,1,D]\n",
    "        Q = self.q.expand(x.size(0), 1, -1)        # [B,1,D]\n",
    "        # Scores: [B,1,T]\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2)) / self.scale # [B,1,T]\n",
    "        # Mask PADs by setting to -inf before softmax\n",
    "        mask_ = mask.unsqueeze(1)                  # [B,T] -> [B,1,T]\n",
    "        scores = scores.masked_fill(~mask_, float(\"-inf\")) # ~mask_ is True exactly at PAD positions\n",
    "        attn = torch.softmax(scores, dim=-1)       # [B,1,T], gives probability 0 to PAD positions\n",
    "        # Weighted sum over tokens\n",
    "        out = torch.matmul(attn, V).squeeze(1)     # [B,1,D] -> [B,D]\n",
    "        return out, attn.squeeze(1)                # return weights for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536ccde",
   "metadata": {},
   "source": [
    "Contrast with self-attention in a Transformer encoder: there, Q, K, V are all projections of the tokens themselves. Here, Q is a single learned vector, while K and V come from tokens—so it’s attention pooling, not full self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b424fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Head vs Linear Head\n",
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1)\n",
    "    def forward(self, x):                          # x: [B,D]\n",
    "        return self.fc(x).squeeze(-1)\n",
    "\n",
    "class CosineHead(nn.Module):\n",
    "    def __init__(self, dim: int, scale: float = 20.0, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(1, dim))  # single-class weight (binary)\n",
    "        self.b = nn.Parameter(torch.zeros(1)) if bias else None\n",
    "        self.scale = nn.Parameter(torch.tensor(float(scale)))  # learnable temperature\n",
    "    def forward(self, x):                           # x: [B,D]\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        w = F.normalize(self.w, dim=-1)\n",
    "        logit = self.scale * (x @ w.t()).squeeze(-1)\n",
    "        if self.b is not None:\n",
    "            logit = logit + self.b\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25dbfcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence encoder with selectable pooling + head\n",
    "class TinySentenceClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_id,\n",
    "                 pool: str = \"attention\", head: str = \"cosine\",\n",
    "                 trainable_embed: bool = True):\n",
    "        super().__init__()\n",
    "        cfg = EmbeddingModelConfig(vocab_size=vocab_size, embed_dim=embed_dim, pad_id=pad_id)\n",
    "        self.embedding = nn.Embedding(cfg.vocab_size, cfg.embed_dim, padding_idx=cfg.pad_id)\n",
    "        self.embedding.weight.requires_grad=trainable_embed\n",
    "        self.attn = True if pool =='attention' else False\n",
    "        if pool==\"mean\":\n",
    "            self.pool = MeanPooler()\n",
    "        elif pool==\"attention\":\n",
    "            self.pool = ScaledDotAttentionPooler(dim=embed_dim)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        if head == \"linear\":\n",
    "            self.head = LinearHead(embed_dim)\n",
    "        elif head==\"cosine\":\n",
    "            self.head = CosineHead(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        embed = self.embedding(x)\n",
    "        if self.attn:\n",
    "            embed, attn_w = self.pool(embed, mask)\n",
    "        else:\n",
    "            embed = self.pool(embed, mask)\n",
    "            attn_w = None\n",
    "        embed = self.norm(embed)\n",
    "        y = self.head(embed).squeeze(-1)\n",
    "        return y, attn_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68b8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences: List[str], vocab: Dict[str,int], pad_id: int):\n",
    "    tok = [tokenize(s) for s in sentences]\n",
    "    ids = [numericalize(t, vocab) for t in tok]\n",
    "    return pad_batch(ids, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37900d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 32, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VOCAB), EMBED_DIM, VOCAB[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60f80043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinySentenceClassifier(\n",
       "  (embedding): Embedding(33, 32, padding_idx=0)\n",
       "  (pool): ScaledDotAttentionPooler(\n",
       "    (Wk): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (Wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "  )\n",
       "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): CosineHead()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect attention weights on a sample\n",
    "pad_id = VOCAB[PAD_TOKEN]\n",
    "X, M = make_batch([\"representation learning is not fun\"], VOCAB, pad_id)\n",
    "probe = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                                pad_id=pad_id, pool=\"attention\", head=\"cosine\",\n",
    "                                trainable_embed=True).to(DEVICE)\n",
    "probe.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b73dd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights over tokens:\n",
      "tensor([-1.1906], device='mps:0')\n",
      "representation : 0.203\n",
      "    learning : 0.413\n",
      "          is : 0.194\n",
      "         not : 0.077\n",
      "         fun : 0.113\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits, attn = probe(X.to(DEVICE), M.to(DEVICE))\n",
    "    print(\"Attention weights over tokens:\")\n",
    "    toks = [ID2TOK[i] for i in X[0].tolist()]\n",
    "    print(logits)\n",
    "    for t, w in zip(toks, attn[0].cpu().tolist()):\n",
    "        print(f\"{t:>12s} : {w:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny supervised setup (greetings vs food)\n",
    "greeting = [\n",
    "    \"hello there\",\"good morning\",\"hi friend\",\n",
    "    \"good evening\",\"hey buddy\",\"how are you\"\n",
    "]\n",
    "food = [\n",
    "    \"i love pizza\",\"pasta is tasty\",\"eating an apple\",\n",
    "    \"the sandwich is good\",\"fresh salad\",\"i like sushi\"\n",
    "]\n",
    "SUP = greeting + food\n",
    "LAB = [0]*len(greeting) + [1]*len(food)\n",
    "\n",
    "X_all, M_all = make_batch(SUP, VOCAB, VOCAB[PAD_TOKEN])\n",
    "y_all = torch.tensor(LAB, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c2e1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop (full-batch for simplicity)\n",
    "def evaluate(model, X, M, y, thr=0.5):\n",
    "    # loss, acc, brier, margin\n",
    "    model.eval()\n",
    "    logits, _ = model(X, M)\n",
    "    loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    acc = ((probs>=thr).float()==y).float().mean().item()\n",
    "    brier = (probs - y).pow(2).mean().item()\n",
    "    margin = (probs - 0.5).abs().mean().item()\n",
    "    return loss.item(), acc, brier, margin\n",
    "\n",
    "\n",
    "def train_full_batch(model, X, M, y, epochs=60, lr_head=3e-3, lr_emb=3e-4, warm_start_epochs=8, freeze_embeddings=False):\n",
    "    X, M, y = X.to(DEVICE), M.to(DEVICE), y.to(DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    embed_parameters = model.embedding.parameters()\n",
    "\n",
    "    # Stage 1: train head only\n",
    "    for p in embed_parameters: p.requires_grad = False\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=lr_head, weight_decay=1e-4)\n",
    "    for e in range(warm_start_epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(X, M)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        L, A, B, G = evaluate(model, X, M, y)\n",
    "        print(f\"[Warm start epoch {e}] loss={L:.4f} acc={A:.3f} brier={B:.4f} margin={G:.3f}\")\n",
    "    \n",
    "    # Stage 2: train all params\n",
    "    if freeze_embeddings:\n",
    "        opt = torch.optim.Adam([p for p in model.parameters() if p not in embed_parameters], lr_head, weight_decay=1e-4)\n",
    "    else:\n",
    "        for p in embed_parameters: p.requires_grad = True\n",
    "        opt = torch.optim.Adam([{\"params\": [p for p in embed_parameters], \"lr\": lr_emb},\n",
    "                                {\"params\": [p for p in model.parameters() if p not in embed_parameters], \"lr\": lr_head}]\n",
    "                            , weight_decay=1e-4)\n",
    "    \n",
    "    for e in range(warm_start_epochs, epochs):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(X, M)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if e %5==0 or e==epochs-1:\n",
    "            L, A, B, G = evaluate(model, X, M, y)\n",
    "            print(f\"[Train epoch {e}] loss={L:.4f} acc={A:.3f} brier={B:.4f} margin={G:.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1281ff",
   "metadata": {},
   "source": [
    "## Try a cosine head vs a linear head for tiny classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49b2b63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16, 31,  0,  0],\n",
       "         [ 3, 22,  0,  0],\n",
       "         [18, 15,  0,  0],\n",
       "         [ 3, 13,  0,  0],\n",
       "         [17, 11,  0,  0],\n",
       "         [19, 10, 32,  0],\n",
       "         [ 5, 21, 25,  0],\n",
       "         [24,  2, 29,  0],\n",
       "         [12,  8,  9,  0],\n",
       "         [30, 27,  2,  3],\n",
       "         [14, 26,  0,  0],\n",
       "         [ 5, 20, 28,  0]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e809d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warm start epoch 0] loss=0.2989 acc=0.833 brier=0.1019 margin=0.412\n",
      "[Warm start epoch 1] loss=0.0834 acc=1.000 brier=0.0199 margin=0.431\n",
      "[Warm start epoch 2] loss=0.0272 acc=1.000 brier=0.0017 margin=0.474\n",
      "[Warm start epoch 3] loss=0.0208 acc=1.000 brier=0.0012 margin=0.480\n",
      "[Warm start epoch 4] loss=0.0247 acc=1.000 brier=0.0021 margin=0.476\n",
      "[Warm start epoch 5] loss=0.0316 acc=1.000 brier=0.0040 margin=0.471\n",
      "[Warm start epoch 6] loss=0.0369 acc=1.000 brier=0.0057 margin=0.466\n",
      "[Warm start epoch 7] loss=0.0368 acc=1.000 brier=0.0057 margin=0.466\n",
      "[Train epoch 10] loss=0.0031 acc=1.000 brier=0.0000 margin=0.497\n",
      "[Train epoch 15] loss=0.0016 acc=1.000 brier=0.0000 margin=0.498\n",
      "[Train epoch 20] loss=0.0004 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 25] loss=0.0001 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 30] loss=0.0001 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 35] loss=0.0001 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 40] loss=0.0001 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 45] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 50] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 55] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 59] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n"
     ]
    }
   ],
   "source": [
    "# Train on the tiny supervised set\n",
    "model_cos = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                            pad_id=pad_id, pool=\"attention\", head=\"cosine\",\n",
    "                            trainable_embed=True)\n",
    "model_cos = train_full_batch(model_cos, X_all, M_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f833f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warm start epoch 0] loss=0.5954 acc=0.667 brier=0.2042 margin=0.155\n",
      "[Warm start epoch 1] loss=0.5091 acc=0.750 brier=0.1656 margin=0.149\n",
      "[Warm start epoch 2] loss=0.4319 acc=0.917 brier=0.1306 margin=0.166\n",
      "[Warm start epoch 3] loss=0.3618 acc=1.000 brier=0.0998 margin=0.206\n",
      "[Warm start epoch 4] loss=0.2998 acc=1.000 brier=0.0746 margin=0.249\n",
      "[Warm start epoch 5] loss=0.2477 acc=1.000 brier=0.0558 margin=0.288\n",
      "[Warm start epoch 6] loss=0.2062 acc=1.000 brier=0.0425 margin=0.321\n",
      "[Warm start epoch 7] loss=0.1733 acc=1.000 brier=0.0329 margin=0.347\n",
      "[Train epoch 10] loss=0.0875 acc=1.000 brier=0.0089 margin=0.417\n",
      "[Train epoch 15] loss=0.0322 acc=1.000 brier=0.0012 margin=0.468\n",
      "[Train epoch 20] loss=0.0149 acc=1.000 brier=0.0002 margin=0.485\n",
      "[Train epoch 25] loss=0.0084 acc=1.000 brier=0.0001 margin=0.492\n",
      "[Train epoch 30] loss=0.0055 acc=1.000 brier=0.0000 margin=0.494\n",
      "[Train epoch 35] loss=0.0040 acc=1.000 brier=0.0000 margin=0.496\n",
      "[Train epoch 40] loss=0.0031 acc=1.000 brier=0.0000 margin=0.497\n",
      "[Train epoch 45] loss=0.0026 acc=1.000 brier=0.0000 margin=0.497\n",
      "[Train epoch 50] loss=0.0022 acc=1.000 brier=0.0000 margin=0.498\n",
      "[Train epoch 55] loss=0.0019 acc=1.000 brier=0.0000 margin=0.498\n",
      "[Train epoch 59] loss=0.0018 acc=1.000 brier=0.0000 margin=0.498\n"
     ]
    }
   ],
   "source": [
    "model_lin = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                            pad_id=pad_id, pool=\"attention\", head=\"linear\",\n",
    "                            trainable_embed=True)\n",
    "model_lin = train_full_batch(model_lin, X_all, M_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55e3af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warm start epoch 0] loss=1.9397 acc=0.750 brier=0.2839 margin=0.380\n",
      "[Warm start epoch 1] loss=1.9388 acc=0.750 brier=0.2835 margin=0.380\n",
      "[Warm start epoch 2] loss=1.9387 acc=0.750 brier=0.2830 margin=0.381\n",
      "[Warm start epoch 3] loss=1.9390 acc=0.750 brier=0.2825 margin=0.381\n",
      "[Warm start epoch 4] loss=1.9394 acc=0.750 brier=0.2820 margin=0.382\n",
      "[Warm start epoch 5] loss=1.9399 acc=0.750 brier=0.2815 margin=0.382\n",
      "[Warm start epoch 6] loss=1.9406 acc=0.750 brier=0.2810 margin=0.383\n",
      "[Warm start epoch 7] loss=1.9412 acc=0.750 brier=0.2805 margin=0.384\n",
      "[Train epoch 10] loss=1.9279 acc=0.750 brier=0.2806 margin=0.384\n",
      "[Train epoch 15] loss=1.9155 acc=0.750 brier=0.2788 margin=0.386\n",
      "[Train epoch 20] loss=1.9051 acc=0.750 brier=0.2772 margin=0.388\n",
      "[Train epoch 25] loss=1.8953 acc=0.750 brier=0.2759 margin=0.389\n",
      "[Train epoch 30] loss=1.8860 acc=0.750 brier=0.2747 margin=0.391\n",
      "[Train epoch 35] loss=1.8766 acc=0.750 brier=0.2738 margin=0.393\n",
      "[Train epoch 40] loss=1.8666 acc=0.750 brier=0.2730 margin=0.394\n",
      "[Train epoch 45] loss=1.8560 acc=0.750 brier=0.2725 margin=0.395\n",
      "[Train epoch 50] loss=1.8446 acc=0.750 brier=0.2720 margin=0.396\n",
      "[Train epoch 55] loss=1.8323 acc=0.750 brier=0.2718 margin=0.396\n",
      "[Train epoch 59] loss=1.8217 acc=0.750 brier=0.2716 margin=0.397\n"
     ]
    }
   ],
   "source": [
    "model_cos0 = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                            pad_id=pad_id, pool=\"mean\", head=\"cosine\",\n",
    "                            trainable_embed=True)\n",
    "model_cos0 = train_full_batch(model_cos0, X_all, M_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da597cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warm start epoch 0] loss=0.7442 acc=0.667 brier=0.2725 margin=0.148\n",
      "[Warm start epoch 1] loss=0.7353 acc=0.667 brier=0.2685 margin=0.149\n",
      "[Warm start epoch 2] loss=0.7267 acc=0.667 brier=0.2645 margin=0.150\n",
      "[Warm start epoch 3] loss=0.7182 acc=0.667 brier=0.2606 margin=0.151\n",
      "[Warm start epoch 4] loss=0.7099 acc=0.667 brier=0.2568 margin=0.152\n",
      "[Warm start epoch 5] loss=0.7017 acc=0.667 brier=0.2530 margin=0.153\n",
      "[Warm start epoch 6] loss=0.6937 acc=0.667 brier=0.2493 margin=0.154\n",
      "[Warm start epoch 7] loss=0.6859 acc=0.667 brier=0.2456 margin=0.155\n",
      "[Train epoch 10] loss=0.6621 acc=0.667 brier=0.2345 margin=0.157\n",
      "[Train epoch 15] loss=0.6260 acc=0.667 brier=0.2177 margin=0.160\n",
      "[Train epoch 20] loss=0.5926 acc=0.667 brier=0.2023 margin=0.162\n",
      "[Train epoch 25] loss=0.5616 acc=0.750 brier=0.1882 margin=0.164\n",
      "[Train epoch 30] loss=0.5328 acc=0.750 brier=0.1751 margin=0.167\n",
      "[Train epoch 35] loss=0.5060 acc=0.750 brier=0.1629 margin=0.170\n",
      "[Train epoch 40] loss=0.4811 acc=0.833 brier=0.1516 margin=0.173\n",
      "[Train epoch 45] loss=0.4577 acc=0.833 brier=0.1411 margin=0.180\n",
      "[Train epoch 50] loss=0.4358 acc=0.833 brier=0.1313 margin=0.186\n",
      "[Train epoch 55] loss=0.4152 acc=0.833 brier=0.1221 margin=0.192\n",
      "[Train epoch 59] loss=0.3995 acc=0.917 brier=0.1152 margin=0.199\n"
     ]
    }
   ],
   "source": [
    "model_lin0 = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                            pad_id=pad_id, pool=\"mean\", head=\"linear\",\n",
    "                            trainable_embed=True)\n",
    "model_lin0 = train_full_batch(model_lin0, X_all, M_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b5c4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('embedding.weight',\n",
       " Parameter containing:\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3877,  1.5380,  1.3613,  ..., -0.3128, -0.9121, -0.5456],\n",
       "         [ 1.4923, -0.6918,  2.1975,  ..., -1.5608, -0.9082, -0.7317],\n",
       "         ...,\n",
       "         [-1.3483, -0.9353, -0.5343,  ..., -0.4552,  0.6070, -0.0176],\n",
       "         [-0.1786, -0.1068,  0.5567,  ..., -0.7250, -1.7054, -2.2150],\n",
       "         [ 0.0138,  0.8519,  1.1709,  ...,  0.6909,  2.5133,  1.1747]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cos_no_embed = TinySentenceClassifier(vocab_size=len(VOCAB), embed_dim=EMBED_DIM,\n",
    "                            pad_id=pad_id, pool=\"attention\", head=\"cosine\",\n",
    "                            trainable_embed=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6bc8327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warm start epoch 0] loss=0.7461 acc=0.750 brier=0.2464 margin=0.315\n",
      "[Warm start epoch 1] loss=0.3952 acc=0.750 brier=0.1375 margin=0.326\n",
      "[Warm start epoch 2] loss=0.2006 acc=1.000 brier=0.0508 margin=0.334\n",
      "[Warm start epoch 3] loss=0.1113 acc=1.000 brier=0.0190 margin=0.400\n",
      "[Warm start epoch 4] loss=0.0683 acc=1.000 brier=0.0085 margin=0.436\n",
      "[Warm start epoch 5] loss=0.0442 acc=1.000 brier=0.0042 margin=0.458\n",
      "[Warm start epoch 6] loss=0.0292 acc=1.000 brier=0.0021 margin=0.472\n",
      "[Warm start epoch 7] loss=0.0197 acc=1.000 brier=0.0010 margin=0.481\n",
      "[Train epoch 10] loss=0.0022 acc=1.000 brier=0.0000 margin=0.498\n",
      "[Train epoch 15] loss=0.0003 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 20] loss=0.0001 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 25] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 30] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 35] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 40] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 45] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 50] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 55] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n",
      "[Train epoch 59] loss=0.0000 acc=1.000 brier=0.0000 margin=0.500\n"
     ]
    }
   ],
   "source": [
    "model_cos_no_embed = train_full_batch(model_cos_no_embed, X_all, M_all, y_all, freeze_embeddings=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "representation_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
